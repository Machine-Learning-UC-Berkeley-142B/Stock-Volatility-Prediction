{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yf = pd.read_csv('data/all_stock_data_transformed_horizontally.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>...</th>\n",
       "      <th>WILLR_14_lag_3</th>\n",
       "      <th>WILLR_14_lag_4</th>\n",
       "      <th>WILLR_14_lag_5</th>\n",
       "      <th>OBV_lag_1</th>\n",
       "      <th>OBV_lag_2</th>\n",
       "      <th>OBV_lag_3</th>\n",
       "      <th>OBV_lag_4</th>\n",
       "      <th>OBV_lag_5</th>\n",
       "      <th>gain_loss_pct</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>-0.497876</td>\n",
       "      <td>-0.496862</td>\n",
       "      <td>-0.497650</td>\n",
       "      <td>-0.496733</td>\n",
       "      <td>-0.450456</td>\n",
       "      <td>36.113739</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.805879</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800159</td>\n",
       "      <td>0.954954</td>\n",
       "      <td>0.967344</td>\n",
       "      <td>-6.249355</td>\n",
       "      <td>-6.249623</td>\n",
       "      <td>-6.250280</td>\n",
       "      <td>-6.248141</td>\n",
       "      <td>-6.246777</td>\n",
       "      <td>1.443757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-08</td>\n",
       "      <td>-0.497349</td>\n",
       "      <td>-0.496377</td>\n",
       "      <td>-0.497575</td>\n",
       "      <td>-0.496634</td>\n",
       "      <td>-0.450371</td>\n",
       "      <td>75.058095</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.215210</td>\n",
       "      <td>-0.005819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970425</td>\n",
       "      <td>0.800166</td>\n",
       "      <td>0.954961</td>\n",
       "      <td>-6.298288</td>\n",
       "      <td>-6.249074</td>\n",
       "      <td>-6.249342</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>-6.247860</td>\n",
       "      <td>0.054783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-15</td>\n",
       "      <td>-0.497347</td>\n",
       "      <td>-0.496663</td>\n",
       "      <td>-0.497097</td>\n",
       "      <td>-0.496604</td>\n",
       "      <td>-0.450346</td>\n",
       "      <td>23.887793</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.294118</td>\n",
       "      <td>-0.707636</td>\n",
       "      <td>...</td>\n",
       "      <td>1.034995</td>\n",
       "      <td>0.970432</td>\n",
       "      <td>0.800173</td>\n",
       "      <td>-6.196910</td>\n",
       "      <td>-6.298005</td>\n",
       "      <td>-6.248793</td>\n",
       "      <td>-6.249062</td>\n",
       "      <td>-6.249719</td>\n",
       "      <td>0.139038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-22</td>\n",
       "      <td>-0.497229</td>\n",
       "      <td>-0.496422</td>\n",
       "      <td>-0.496797</td>\n",
       "      <td>-0.496242</td>\n",
       "      <td>-0.450036</td>\n",
       "      <td>25.660756</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.101576</td>\n",
       "      <td>-1.441405</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.908937</td>\n",
       "      <td>1.035001</td>\n",
       "      <td>0.970439</td>\n",
       "      <td>-6.164441</td>\n",
       "      <td>-6.196632</td>\n",
       "      <td>-6.297723</td>\n",
       "      <td>-6.248513</td>\n",
       "      <td>-6.248781</td>\n",
       "      <td>0.860787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-29</td>\n",
       "      <td>-0.496854</td>\n",
       "      <td>-0.495821</td>\n",
       "      <td>-0.496372</td>\n",
       "      <td>-0.495736</td>\n",
       "      <td>-0.449605</td>\n",
       "      <td>31.220490</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.687041</td>\n",
       "      <td>-2.153361</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.908791</td>\n",
       "      <td>-1.908927</td>\n",
       "      <td>1.035009</td>\n",
       "      <td>-6.129584</td>\n",
       "      <td>-6.164164</td>\n",
       "      <td>-6.196353</td>\n",
       "      <td>-6.297440</td>\n",
       "      <td>-6.248232</td>\n",
       "      <td>1.184667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393423</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>1.300387</td>\n",
       "      <td>1.248745</td>\n",
       "      <td>1.299685</td>\n",
       "      <td>1.283046</td>\n",
       "      <td>1.344140</td>\n",
       "      <td>-0.214561</td>\n",
       "      <td>BIO</td>\n",
       "      <td>-1.762781</td>\n",
       "      <td>-1.712488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939853</td>\n",
       "      <td>-1.357087</td>\n",
       "      <td>-1.820190</td>\n",
       "      <td>1.223875</td>\n",
       "      <td>1.223835</td>\n",
       "      <td>1.223817</td>\n",
       "      <td>1.223761</td>\n",
       "      <td>1.223713</td>\n",
       "      <td>-0.200466</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393424</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>1.277312</td>\n",
       "      <td>1.280437</td>\n",
       "      <td>1.313742</td>\n",
       "      <td>1.266341</td>\n",
       "      <td>1.327306</td>\n",
       "      <td>-0.214341</td>\n",
       "      <td>BIO</td>\n",
       "      <td>-1.719633</td>\n",
       "      <td>-1.736968</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.046805</td>\n",
       "      <td>-0.939844</td>\n",
       "      <td>-1.357077</td>\n",
       "      <td>1.223863</td>\n",
       "      <td>1.223840</td>\n",
       "      <td>1.223800</td>\n",
       "      <td>1.223782</td>\n",
       "      <td>1.223726</td>\n",
       "      <td>-0.125076</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393425</th>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>1.278071</td>\n",
       "      <td>1.335034</td>\n",
       "      <td>1.255892</td>\n",
       "      <td>1.277924</td>\n",
       "      <td>1.338978</td>\n",
       "      <td>-0.208835</td>\n",
       "      <td>BIO</td>\n",
       "      <td>-1.656665</td>\n",
       "      <td>-1.743115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776454</td>\n",
       "      <td>-1.046796</td>\n",
       "      <td>-0.939834</td>\n",
       "      <td>1.223851</td>\n",
       "      <td>1.223828</td>\n",
       "      <td>1.223805</td>\n",
       "      <td>1.223764</td>\n",
       "      <td>1.223747</td>\n",
       "      <td>0.007847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393426</th>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>1.303308</td>\n",
       "      <td>1.336508</td>\n",
       "      <td>1.331884</td>\n",
       "      <td>1.363023</td>\n",
       "      <td>1.424733</td>\n",
       "      <td>-0.212919</td>\n",
       "      <td>BIO</td>\n",
       "      <td>-1.518174</td>\n",
       "      <td>-1.718477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.872208</td>\n",
       "      <td>-0.776445</td>\n",
       "      <td>-1.046787</td>\n",
       "      <td>1.223871</td>\n",
       "      <td>1.223816</td>\n",
       "      <td>1.223793</td>\n",
       "      <td>1.223770</td>\n",
       "      <td>1.223729</td>\n",
       "      <td>0.732549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393427</th>\n",
       "      <td>2023-12-23</td>\n",
       "      <td>1.370607</td>\n",
       "      <td>1.347733</td>\n",
       "      <td>1.401208</td>\n",
       "      <td>1.375538</td>\n",
       "      <td>1.437344</td>\n",
       "      <td>-0.220221</td>\n",
       "      <td>BIO</td>\n",
       "      <td>-1.382032</td>\n",
       "      <td>-1.669714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.850541</td>\n",
       "      <td>-0.872199</td>\n",
       "      <td>-0.776436</td>\n",
       "      <td>1.223885</td>\n",
       "      <td>1.223835</td>\n",
       "      <td>1.223781</td>\n",
       "      <td>1.223758</td>\n",
       "      <td>1.223734</td>\n",
       "      <td>0.067633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393428 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date      Open      High       Low     Close  Adj Close  \\\n",
       "0       2005-01-01 -0.497876 -0.496862 -0.497650 -0.496733  -0.450456   \n",
       "1       2005-01-08 -0.497349 -0.496377 -0.497575 -0.496634  -0.450371   \n",
       "2       2005-01-15 -0.497347 -0.496663 -0.497097 -0.496604  -0.450346   \n",
       "3       2005-01-22 -0.497229 -0.496422 -0.496797 -0.496242  -0.450036   \n",
       "4       2005-01-29 -0.496854 -0.495821 -0.496372 -0.495736  -0.449605   \n",
       "...            ...       ...       ...       ...       ...        ...   \n",
       "393423  2023-11-25  1.300387  1.248745  1.299685  1.283046   1.344140   \n",
       "393424  2023-12-02  1.277312  1.280437  1.313742  1.266341   1.327306   \n",
       "393425  2023-12-09  1.278071  1.335034  1.255892  1.277924   1.338978   \n",
       "393426  2023-12-16  1.303308  1.336508  1.331884  1.363023   1.424733   \n",
       "393427  2023-12-23  1.370607  1.347733  1.401208  1.375538   1.437344   \n",
       "\n",
       "           Volume Ticker      macd  macd_signal  ...  WILLR_14_lag_3  \\\n",
       "0       36.113739   AAPL -0.805879     0.583648  ...        0.800159   \n",
       "1       75.058095   AAPL -2.215210    -0.005819  ...        0.970425   \n",
       "2       23.887793   AAPL -3.294118    -0.707636  ...        1.034995   \n",
       "3       25.660756   AAPL -4.101576    -1.441405  ...       -1.908937   \n",
       "4       31.220490   AAPL -4.687041    -2.153361  ...       -1.908791   \n",
       "...           ...    ...       ...          ...  ...             ...   \n",
       "393423  -0.214561    BIO -1.762781    -1.712488  ...       -0.939853   \n",
       "393424  -0.214341    BIO -1.719633    -1.736968  ...       -1.046805   \n",
       "393425  -0.208835    BIO -1.656665    -1.743115  ...       -0.776454   \n",
       "393426  -0.212919    BIO -1.518174    -1.718477  ...       -0.872208   \n",
       "393427  -0.220221    BIO -1.382032    -1.669714  ...       -0.850541   \n",
       "\n",
       "        WILLR_14_lag_4  WILLR_14_lag_5  OBV_lag_1  OBV_lag_2  OBV_lag_3  \\\n",
       "0             0.954954        0.967344  -6.249355  -6.249623  -6.250280   \n",
       "1             0.800166        0.954961  -6.298288  -6.249074  -6.249342   \n",
       "2             0.970432        0.800173  -6.196910  -6.298005  -6.248793   \n",
       "3             1.035001        0.970439  -6.164441  -6.196632  -6.297723   \n",
       "4            -1.908927        1.035009  -6.129584  -6.164164  -6.196353   \n",
       "...                ...             ...        ...        ...        ...   \n",
       "393423       -1.357087       -1.820190   1.223875   1.223835   1.223817   \n",
       "393424       -0.939844       -1.357077   1.223863   1.223840   1.223800   \n",
       "393425       -1.046796       -0.939834   1.223851   1.223828   1.223805   \n",
       "393426       -0.776445       -1.046787   1.223871   1.223816   1.223793   \n",
       "393427       -0.872199       -0.776436   1.223885   1.223835   1.223781   \n",
       "\n",
       "        OBV_lag_4  OBV_lag_5  gain_loss_pct  win  \n",
       "0       -6.248141  -6.246777       1.443757    1  \n",
       "1       -6.250000  -6.247860       0.054783    1  \n",
       "2       -6.249062  -6.249719       0.139038    1  \n",
       "3       -6.248513  -6.248781       0.860787    1  \n",
       "4       -6.297440  -6.248232       1.184667    1  \n",
       "...           ...        ...            ...  ...  \n",
       "393423   1.223761   1.223713      -0.200466   -1  \n",
       "393424   1.223782   1.223726      -0.125076   -1  \n",
       "393425   1.223764   1.223747       0.007847    0  \n",
       "393426   1.223770   1.223729       0.732549    1  \n",
       "393427   1.223758   1.223734       0.067633    1  \n",
       "\n",
       "[393428 rows x 128 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset()\n",
    "valid_dataset = TextDataset(\n",
    "test_dataset = TextDataset(\n",
    "\n",
    "# Define a dataloader\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch \n",
    "class StockLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        all_h, (h, c) = self.lstm(x)\n",
    "        out = self.fc(all_h) # Apply Linear layer to outputs from all the hidden state.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import pyepo\n",
    "from pyepo.model.grb import optGrbModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "m = 50 # change based on number of assets\n",
    "cov = np.cov(np.random.randn(10, m), rowvar=False) # covariance matrix\n",
    "optmodel = pyepo.model.grb.portfolioModel(m, cov) # build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# train model\n",
    "def trainModel(reg, loss_func, method_name, num_epochs=20, lr=1e-2):\n",
    "    # set adam optimizer\n",
    "    optimizer = torch.optim.Adam(reg.parameters(), lr=lr)\n",
    "    # train mode\n",
    "    reg.train()\n",
    "    # init log\n",
    "    train_loss_log = []\n",
    "    loss_log_regret = [pyepo.metric.regret(reg, optmodel, loader_test)]\n",
    "    # init elpased time\n",
    "    elapsed = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # start timing\n",
    "        tick = time.time()\n",
    "        # load data\n",
    "        train_loss = 0\n",
    "        for i, data in enumerate(loader_train):\n",
    "            x, c, w, z = data\n",
    "            # cuda\n",
    "            if torch.cuda.is_available():\n",
    "                x, c, w, z = x.cuda(), c.cuda(), w.cuda(), z.cuda()\n",
    "            # forward pass\n",
    "            cp = reg(x)\n",
    "            if method_name == \"spo+\":\n",
    "                loss = loss_func(cp, c, w, z)\n",
    "            elif method_name == \"mse\":\n",
    "                loss = loss_func(cp, c)\n",
    "            else:\n",
    "                raise ValueError(\"Method name {} not supported\".format(method_name))\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # record time\n",
    "            tock = time.time()\n",
    "            elapsed += tock - tick\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(loader_train)\n",
    "        train_loss_log.append(train_loss)\n",
    "        regret = pyepo.metric.regret(reg, optmodel, loader_test)\n",
    "        loss_log_regret.append(regret)\n",
    "        print(\"Epoch {:2},  Loss: {:9.4f},  Regret: {:7.4f}%\".format(epoch+1, train_loss, regret*100))\n",
    "    print(\"Total Elapsed Time: {:.2f} Sec.\".format(elapsed))\n",
    "    return train_loss_log, loss_log_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Predict-then-Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spop = pyepo.func.SPOPlus(optmodel, processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = #fill in once data is imported\n",
    "EMBEDDING_DIM = #param we can optimize\n",
    "HIDDEN_DIM = #also a param to optimize\n",
    "learning_rate = #another optimizable param\n",
    "epoch = #optimize\n",
    "num_layers = #optimize\n",
    "epochs = 20\n",
    "learning_rate = 2e-3\n",
    "method_name = \"spo+\"\n",
    "\n",
    "# Instantiate the model\n",
    "lstm = StockLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,num_layers=num_layers)\n",
    "loss_log_lstm_spo, loss_log_regret_lstm_spo = trainModel(lstm, loss_func=spop, method_name=method_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
