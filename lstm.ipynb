{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTING GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using device: GPU\n"
     ]
    }
   ],
   "source": [
    "# Check type of GPU available on machine\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", physical_devices)\n",
    "# Use GPU if available\n",
    "device_name = \"GPU\" if tf.config.experimental.list_physical_devices('GPU') else \"cpu\"\n",
    "print(\"Using device:\", device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is meant for multidimentional data\n",
    "def prepare_features_targets(data):\n",
    "    features = data.loc[:, ~data.columns.get_level_values(0).str.contains('gain_loss')]\n",
    "    targets = data.loc[:, data.columns.get_level_values(0).str.contains('gain_loss')]\n",
    "    return features, targets\n",
    "\n",
    "# Build and compile a neural network\n",
    "def build_model(input_shape, output_shape):\n",
    "    with tf.device(device_name):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal',input_shape=(input_shape,)),\n",
    "            tf.keras.layers.Dense(32, activation='relu',kernel_initializer='he_normal'),\n",
    "            tf.keras.layers.Dense(output_shape)\n",
    "        ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Portfolio optimization function with variance consideration and risk aversion\n",
    "def optimize_portfolio(weights, model, features, historical_returns, risk_aversion=0.8):\n",
    "    weights = weights / np.sum(weights)  # Normalize weights\n",
    "    pred_returns = model.predict(np.array([features.iloc[-1]]))[0]\n",
    "    expected_return = np.dot(weights, pred_returns)\n",
    "    covariance_matrix = historical_returns.cov()\n",
    "    portfolio_variance = np.dot(weights.T, np.dot(covariance_matrix, weights))\n",
    "    # Objective function with risk aversion parameter\n",
    "    return -expected_return + risk_aversion * portfolio_variance  # Maximize returns and penalize variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yf = pd.read_csv('data/all_stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yf.set_index('Date', inplace=True)\n",
    "#set index to datetime\n",
    "data_yf.index = pd.to_datetime(data_yf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get symbols (TICKERS)\n",
    "# this needs to execute before transorming the data into multindex\n",
    "symbols = list(data_yf['Ticker'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l3/h56ll3s97r3bp825gbh1f4q80000gq/T/ipykernel_19620/2531892422.py:1: FutureWarning: pivot_table dropped a column because it failed to aggregate. This behavior is deprecated and will raise in a future version of pandas. Select only the columns that can be aggregated.\n",
      "  data_yf = data_yf.pivot_table(index='Date', columns=['Ticker'], aggfunc='mean')\n"
     ]
    }
   ],
   "source": [
    "data_yf = data_yf.pivot_table(index='Date', columns=['Ticker'], aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_yf.copy()\n",
    "#keep only float columns for model\n",
    "float_columns = data.select_dtypes(include=['float64']).columns\n",
    "data = data[float_columns]\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">Adj Close</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>A</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACGL</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>...</th>\n",
       "      <th>WRB</th>\n",
       "      <th>WST</th>\n",
       "      <th>WTW</th>\n",
       "      <th>WY</th>\n",
       "      <th>WYNN</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XOM</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-01</th>\n",
       "      <td>-0.358229</td>\n",
       "      <td>-0.430459</td>\n",
       "      <td>-0.354121</td>\n",
       "      <td>-0.412590</td>\n",
       "      <td>-0.328286</td>\n",
       "      <td>-0.269412</td>\n",
       "      <td>-0.308205</td>\n",
       "      <td>-0.356685</td>\n",
       "      <td>-0.314928</td>\n",
       "      <td>-0.241575</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-08</th>\n",
       "      <td>-0.360501</td>\n",
       "      <td>-0.430378</td>\n",
       "      <td>-0.356287</td>\n",
       "      <td>-0.412881</td>\n",
       "      <td>-0.329586</td>\n",
       "      <td>-0.270975</td>\n",
       "      <td>-0.307880</td>\n",
       "      <td>-0.356429</td>\n",
       "      <td>-0.315498</td>\n",
       "      <td>-0.256458</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-15</th>\n",
       "      <td>-0.361362</td>\n",
       "      <td>-0.430353</td>\n",
       "      <td>-0.356312</td>\n",
       "      <td>-0.412704</td>\n",
       "      <td>-0.334178</td>\n",
       "      <td>-0.274270</td>\n",
       "      <td>-0.312100</td>\n",
       "      <td>-0.353692</td>\n",
       "      <td>-0.314272</td>\n",
       "      <td>-0.258502</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-22</th>\n",
       "      <td>-0.362016</td>\n",
       "      <td>-0.430053</td>\n",
       "      <td>-0.357549</td>\n",
       "      <td>-0.413228</td>\n",
       "      <td>-0.332390</td>\n",
       "      <td>-0.278788</td>\n",
       "      <td>-0.311126</td>\n",
       "      <td>-0.352232</td>\n",
       "      <td>-0.313217</td>\n",
       "      <td>-0.274180</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-29</th>\n",
       "      <td>-0.359124</td>\n",
       "      <td>-0.429635</td>\n",
       "      <td>-0.357113</td>\n",
       "      <td>-0.411782</td>\n",
       "      <td>-0.331862</td>\n",
       "      <td>-0.254610</td>\n",
       "      <td>-0.301101</td>\n",
       "      <td>-0.348508</td>\n",
       "      <td>-0.310508</td>\n",
       "      <td>-0.258275</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-02</th>\n",
       "      <td>0.284105</td>\n",
       "      <td>0.674437</td>\n",
       "      <td>0.151772</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>1.465805</td>\n",
       "      <td>3.030413</td>\n",
       "      <td>0.609377</td>\n",
       "      <td>-0.021198</td>\n",
       "      <td>0.851294</td>\n",
       "      <td>0.836689</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-09</th>\n",
       "      <td>0.338371</td>\n",
       "      <td>0.684994</td>\n",
       "      <td>0.167418</td>\n",
       "      <td>-0.019147</td>\n",
       "      <td>1.504839</td>\n",
       "      <td>2.886457</td>\n",
       "      <td>0.699882</td>\n",
       "      <td>-0.009938</td>\n",
       "      <td>0.886618</td>\n",
       "      <td>0.940579</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-16</th>\n",
       "      <td>0.354174</td>\n",
       "      <td>0.662460</td>\n",
       "      <td>0.177210</td>\n",
       "      <td>-0.016760</td>\n",
       "      <td>1.562938</td>\n",
       "      <td>2.966420</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>0.868705</td>\n",
       "      <td>0.943250</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-23</th>\n",
       "      <td>0.351116</td>\n",
       "      <td>0.656387</td>\n",
       "      <td>0.183063</td>\n",
       "      <td>-0.014316</td>\n",
       "      <td>1.542970</td>\n",
       "      <td>2.954201</td>\n",
       "      <td>0.686759</td>\n",
       "      <td>-0.029812</td>\n",
       "      <td>0.880007</td>\n",
       "      <td>0.947342</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows × 18659 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Adj Close                                                    \\\n",
       "Ticker             A      AAPL       ABT      ACGL       ACN      ADBE   \n",
       "Date                                                                     \n",
       "2005-01-01 -0.358229 -0.430459 -0.354121 -0.412590 -0.328286 -0.269412   \n",
       "2005-01-08 -0.360501 -0.430378 -0.356287 -0.412881 -0.329586 -0.270975   \n",
       "2005-01-15 -0.361362 -0.430353 -0.356312 -0.412704 -0.334178 -0.274270   \n",
       "2005-01-22 -0.362016 -0.430053 -0.357549 -0.413228 -0.332390 -0.278788   \n",
       "2005-01-29 -0.359124 -0.429635 -0.357113 -0.411782 -0.331862 -0.254610   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2023-12-02  0.284105  0.674437  0.151772  0.014270  1.465805  3.030413   \n",
       "2023-12-09  0.338371  0.684994  0.167418 -0.019147  1.504839  2.886457   \n",
       "2023-12-16  0.354174  0.662460  0.177210 -0.016760  1.562938  2.966420   \n",
       "2023-12-23  0.351116  0.656387  0.183063 -0.014316  1.542970  2.954201   \n",
       "2023-12-30       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                                                    ...  win                 \\\n",
       "Ticker           ADI       ADM       ADP      ADSK  ...  WRB  WST  WTW   WY   \n",
       "Date                                                ...                       \n",
       "2005-01-01 -0.308205 -0.356685 -0.314928 -0.241575  ... -1.0  1.0 -1.0  0.0   \n",
       "2005-01-08 -0.307880 -0.356429 -0.315498 -0.256458  ...  1.0  0.0 -1.0 -1.0   \n",
       "2005-01-15 -0.312100 -0.353692 -0.314272 -0.258502  ... -1.0  1.0 -1.0 -1.0   \n",
       "2005-01-22 -0.311126 -0.352232 -0.313217 -0.274180  ... -1.0 -1.0 -1.0 -1.0   \n",
       "2005-01-29 -0.301101 -0.348508 -0.310508 -0.258275  ...  1.0  1.0  1.0  1.0   \n",
       "...              ...       ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "2023-12-02  0.609377 -0.021198  0.851294  0.836689  ... -1.0 -1.0 -1.0 -1.0   \n",
       "2023-12-09  0.699882 -0.009938  0.886618  0.940579  ... -1.0  1.0 -1.0  1.0   \n",
       "2023-12-16  0.678500 -0.034091  0.868705  0.943250  ... -1.0 -1.0  1.0  1.0   \n",
       "2023-12-23  0.686759 -0.029812  0.880007  0.947342  ...  1.0 -1.0  0.0  1.0   \n",
       "2023-12-30       NaN       NaN       NaN       NaN  ...  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                                          \n",
       "Ticker     WYNN  XEL  XOM  YUM  ZBH ZBRA  \n",
       "Date                                      \n",
       "2005-01-01 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "2005-01-08 -1.0  1.0  1.0 -1.0 -1.0 -1.0  \n",
       "2005-01-15 -1.0  0.0 -1.0  1.0  1.0  0.0  \n",
       "2005-01-22 -1.0  0.0  1.0 -1.0  0.0 -1.0  \n",
       "2005-01-29  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "...         ...  ...  ...  ...  ...  ...  \n",
       "2023-12-02  0.0  0.0 -1.0 -1.0  0.0 -1.0  \n",
       "2023-12-09  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "2023-12-16  1.0  0.0  0.0  0.0  1.0 -1.0  \n",
       "2023-12-23  1.0  0.0 -1.0  1.0  1.0  1.0  \n",
       "2023-12-30  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[992 rows x 18659 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape: torch.Size([783, 18262])\n",
      "Training Targets shape: torch.Size([783, 397])\n",
      "Validation Features shape: torch.Size([104, 18262])\n",
      "Validation Targets shape: torch.Size([104, 397])\n",
      "Test Features shape: torch.Size([104, 18262])\n",
      "Test Targets shape: torch.Size([104, 397])\n"
     ]
    }
   ],
   "source": [
    "split_date_train = pd.Timestamp('2020-01-01')\n",
    "split_date_val = pd.Timestamp('2021-12-31')\n",
    "\n",
    "train_data = data.loc[data.index < split_date_train]\n",
    "val_data = data.loc[(data.index >= split_date_train) & (data.index < split_date_val)]\n",
    "test_data = data.loc[data.index >= split_date_val]\n",
    "\n",
    "# Prepare features and targets for each dataset\n",
    "features_train, targets_train = prepare_features_targets(train_data)\n",
    "features_val, targets_val = prepare_features_targets(val_data)\n",
    "features_test, targets_test = prepare_features_targets(test_data)\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoader objects\n",
    "features_train = torch.tensor(features_train.values.astype(np.float32))\n",
    "targets_train = torch.tensor(targets_train.values.astype(np.float32))\n",
    "features_val = torch.tensor(features_val.values.astype(np.float32))\n",
    "targets_val = torch.tensor(targets_val.values.astype(np.float32))\n",
    "features_test = torch.tensor(features_test.values.astype(np.float32))\n",
    "targets_test = torch.tensor(targets_test.values.astype(np.float32))\n",
    "\n",
    "train_dataset = TensorDataset(features_train, targets_train)\n",
    "val_dataset = TensorDataset(features_val, targets_val)\n",
    "test_dataset = TensorDataset(features_test, targets_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Training Features shape:\", features_train.shape)\n",
    "print(\"Training Targets shape:\", targets_train.shape)\n",
    "print(\"Validation Features shape:\", features_val.shape)\n",
    "print(\"Validation Targets shape:\", targets_val.shape)\n",
    "print(\"Test Features shape:\", features_test.shape)\n",
    "print(\"Test Targets shape:\", targets_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features shape: torch.Size([783, 1, 18262])\n",
      "New features shape: torch.Size([104, 1, 18262])\n",
      "New features shape: torch.Size([104, 1, 18262])\n"
     ]
    }
   ],
   "source": [
    "features_train = features_train[:, None, :]  # Add a time step dimension\n",
    "print(\"New features shape:\", features_train.shape)\n",
    "\n",
    "features_val = features_val[:, None, :]  # Add a time step dimension\n",
    "print(\"New features shape:\", features_val.shape)\n",
    "\n",
    "features_test = features_test[:, None, :]  # Add a time step dimension\n",
    "print(\"New features shape:\", features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: torch.Size([783, 1, 18262])\n",
      "Training targets shape: torch.Size([783, 397])\n",
      "Validation features shape: torch.Size([104, 1, 18262])\n",
      "Validation targets shape: torch.Size([104, 397])\n",
      "Testing features shape: torch.Size([104, 1, 18262])\n",
      "Testing targets shape: torch.Size([104, 397])\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the training and testing data to confirm\n",
    "print(\"Training features shape:\", features_train.shape)\n",
    "print(\"Training targets shape:\", targets_train.shape)\n",
    "print(\"Validation features shape:\", features_val.shape)\n",
    "print(\"Validation targets shape:\", targets_val.shape)\n",
    "print(\"Testing features shape:\", features_test.shape)\n",
    "print(\"Testing targets shape:\", targets_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MV/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "def build_model(num_features, num_time_steps, output_shape):\n",
    "    model = Sequential()\n",
    "    # Add a LSTM layer only if your data is sequenced; otherwise start with Dense\n",
    "    model.add(LSTM(64, activation = 'relu', input_shape=(num_time_steps, num_features), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, activation = 'relu', return_sequences=False))\n",
    "    model.add(Dense(output_shape, activation='sigmoid'))  # Sigmoid or softmax based on your needs\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Example usage assuming you've correctly shaped your data\n",
    "num_features = features_train.shape[2]  # number of features per timestep\n",
    "num_time_steps = features_train.shape[1]  # number of timesteps per sample\n",
    "output_shape = targets_train.shape[1]  # assuming this is a regression task or multi-label classification\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()\n",
    "# Now define and compile your model again\n",
    "model = build_model(num_features, num_time_steps, output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1.3312 - val_loss: 0.7958\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 1.2644 - val_loss: 0.7023\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.1522 - val_loss: 0.6337\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 1.1160 - val_loss: 0.6056\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.0506 - val_loss: 0.5933\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.0620 - val_loss: 0.5789\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.1369 - val_loss: 0.5658\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.9523 - val_loss: 0.5566\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.9502 - val_loss: 0.5563\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.9056 - val_loss: 0.5466\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    features_train, targets_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5135\n",
      "Validation loss: 1.1915425062179565\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "val_loss = model.evaluate(features_val, targets_val, verbose=1)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWcklEQVR4nO3dd3gU1f7H8fduyqYH0gkQCL0JUqQjIAiColixUVRUroAi18bFBlflqj8sVwRFQcSCiAXRiwURkKqIgPQiJQESQgKkkj6/PxYWloQlgZBJNp/X88xDdvbMzHeJuh/POTPHYhiGgYiIiIibsJpdgIiIiEhZUrgRERERt6JwIyIiIm5F4UZERETcisKNiIiIuBWFGxEREXErCjciIiLiVhRuRERExK0o3IiIiIhbUbgRkRKZNWsWFosFi8XC0qVLi7xvGAYNGjTAYrHQo0ePMr22xWLh+eefL/Vx+/btw2KxMGvWrDJpJyKVg8KNiJRKYGAgM2bMKLJ/2bJl/P333wQGBppQlYjIaQo3IlIqgwYN4ssvvyQtLc1p/4wZM+jUqRMxMTEmVSYiYqdwIyKlcscddwAwZ84cx77U1FS+/PJL7r333mKPOXr0KA899BA1a9bE29ubevXqMX78eHJycpzapaWlcf/99xMaGkpAQADXXHMNO3fuLPacu3bt4s477yQiIgKbzUbTpk15++23y+hT2q1YsYJevXoRGBiIn58fnTt35n//+59Tm6ysLB577DFiY2Px8fEhJCSEdu3aOf397Nmzh9tvv53o6GhsNhuRkZH06tWLDRs2lGm9ImLnaXYBIlK5BAUFccsttzBz5kwefPBBwB50rFYrgwYN4o033nBqn52dTc+ePfn777+ZMGECLVu2ZPny5UyaNIkNGzY4woJhGAwcOJBVq1bx7LPPcsUVV7By5Ur69etXpIatW7fSuXNnYmJimDx5MlFRUfz44488/PDDJCcn89xzz13051y2bBlXX301LVu2ZMaMGdhsNqZOncqAAQOYM2cOgwYNAmDs2LF89NFHvPDCC7Ru3ZrMzEw2b95MSkqK41z9+/enoKCAV155hZiYGJKTk1m1ahXHjx+/6DpFpBiGiEgJfPDBBwZgrF271liyZIkBGJs3bzYMwzCuuOIKY9iwYYZhGEbz5s2N7t27O4575513DMD4/PPPnc738ssvG4Dx008/GYZhGN9//70BGG+++aZTuxdffNEAjOeee86xr2/fvkatWrWM1NRUp7ajRo0yfHx8jKNHjxqGYRh79+41AOODDz5w+dmKa9exY0cjIiLCSE9Pd+zLz883WrRoYdSqVcsoLCw0DMMwWrRoYQwcOPCc505OTjYA44033nBZg4iUHQ1LiUipde/enfr16zNz5kw2bdrE2rVrzzkk9csvv+Dv788tt9zitH/YsGEALF68GIAlS5YAcNdddzm1u/POO51eZ2dns3jxYm688Ub8/PzIz893bP379yc7O5s1a9Zc1OfLzMzkt99+45ZbbiEgIMCx38PDg8GDB3PgwAF27NgBQPv27fn+++956qmnWLp0KSdOnHA6V0hICPXr1+fVV1/ltddeY/369RQWFl5UfSLimsKNiJSaxWLhnnvu4eOPP+add96hUaNGdOvWrdi2KSkpREVFYbFYnPZHRETg6enpGL5JSUnB09OT0NBQp3ZRUVFFzpefn89bb72Fl5eX09a/f38AkpOTL+rzHTt2DMMwqFGjRpH3oqOjHXUA/Pe//+XJJ59k/vz59OzZk5CQEAYOHMiuXbsA+9/V4sWL6du3L6+88gpt2rQhPDychx9+mPT09IuqU0SKp3AjIhdk2LBhJCcn884773DPPfecs11oaCiHDx/GMAyn/UlJSeTn5xMWFuZol5+f7zRXBSAxMdHpdfXq1fHw8GDYsGGsXbu22O1UyLlQ1atXx2q1kpCQUOS9Q4cOATjq9vf3Z8KECWzfvp3ExESmTZvGmjVrGDBggOOYOnXqMGPGDBITE9mxYwePPvooU6dO5fHHH7+oOkWkeAo3InJBatasyeOPP86AAQMYOnToOdv16tWLjIwM5s+f77R/9uzZjvcBevbsCcAnn3zi1O7TTz91eu3n50fPnj1Zv349LVu2pF27dkW2s3t/Ssvf358OHTrw1VdfOQ0zFRYW8vHHH1OrVi0aNWpU5LjIyEiGDRvGHXfcwY4dO8jKyirSplGjRjz99NNcdtll/PnnnxdVp4gUT3dLicgF+89//nPeNkOGDOHtt99m6NCh7Nu3j8suu4wVK1bw0ksv0b9/f3r37g1Anz59uPLKK3niiSfIzMykXbt2rFy5ko8++qjIOd988026du1Kt27d+Mc//kHdunVJT09n9+7dfPvtt/zyyy8X/dkmTZrE1VdfTc+ePXnsscfw9vZm6tSpbN68mTlz5jiG2Tp06MB1111Hy5YtqV69Otu2beOjjz6iU6dO+Pn58ddffzFq1ChuvfVWGjZsiLe3N7/88gt//fUXTz311EXXKSJFKdyIyCXl4+PDkiVLGD9+PK+++ipHjhyhZs2aPPbYY063bFutVhYsWMDYsWN55ZVXyM3NpUuXLixcuJAmTZo4nbNZs2b8+eef/Pvf/+bpp58mKSmJatWq0bBhw4sekjqle/fu/PLLLzz33HMMGzaMwsJCWrVqxYIFC7juuusc7a666ioWLFjA66+/TlZWFjVr1mTIkCGMHz8esM8Zql+/PlOnTiU+Ph6LxUK9evWYPHkyo0ePLpNaRcSZxTh7IFxERESkEtOcGxEREXErCjciIiLiVhRuRERExK0o3IiIiIhbUbgRERERt6JwIyIiIm6lyj3nprCwkEOHDhEYGFhkrRsRERGpmAzDID09nejoaKxW130zVS7cHDp0iNq1a5tdhoiIiFyA+Ph4atWq5bJNlQs3gYGBgP0vJygoyORqREREpCTS0tKoXbu243vclSoXbk4NRQUFBSnciIiIVDIlmVKiCcUiIiLiVhRuRERExK0o3IiIiIhbqXJzbkRExH0UFhaSm5trdhlSRry9vc97m3dJKNyIiEillJuby969eyksLDS7FCkjVquV2NhYvL29L+o8CjciIlLpGIZBQkICHh4e1K5du0z+b1/MdeohuwkJCcTExFzUg3YVbkREpNLJz88nKyuL6Oho/Pz8zC5Hykh4eDiHDh0iPz8fLy+vCz6Poq6IiFQ6BQUFABc9fCEVy6nf56nf74VSuBERkUpLawS6l7L6fSrciIiIiFsxNdz8+uuvDBgwgOjoaCwWC/Pnz3fZfsWKFXTp0oXQ0FB8fX1p0qQJr7/+evkUKyIiUgH16NGDMWPGmF1GhWLqhOLMzExatWrFPffcw80333ze9v7+/owaNYqWLVvi7+/PihUrePDBB/H39+eBBx4oh4pFREQuzPmGXIYOHcqsWbNKfd6vvvrqoibfuiNTw02/fv3o169fidu3bt2a1q1bO17XrVuXr776iuXLl1eIcLN231EaRwUS5KN/yERExFlCQoLj57lz5/Lss8+yY8cOxz5fX1+n9nl5eSUKLSEhIWVXpJuo1HNu1q9fz6pVq+jevbvZpZCckcN9s9Zy1f8tY/76gxiGYXZJIiJSgURFRTm24OBgLBaL43V2djbVqlXj888/p0ePHvj4+PDxxx+TkpLCHXfcQa1atfDz8+Oyyy5jzpw5Tuc9e1iqbt26vPTSS9x7770EBgYSExPD9OnTy/nTmqtShptatWphs9lo164dI0eOZPjw4edsm5OTQ1pamtN2KSSl5RAWYCM5I4cxczdwx3tr2HU4/ZJcS0REnBmGQVZuvilbWf7P7JNPPsnDDz/Mtm3b6Nu3L9nZ2bRt25bvvvuOzZs388ADDzB48GB+++03l+eZPHky7dq1Y/369Tz00EP84x//YPv27WVWZ0VXKR/it3z5cjIyMlizZg1PPfUUDRo04I477ii27aRJk5gwYcIlr6lZdBDfj+nG+8v38tYvu1iz5yj93lzOfV1jebhXQ/xtlfKvWkSkUjiRV0CzZ3805dpbJ/bFz7ts/hs/ZswYbrrpJqd9jz32mOPn0aNH88MPPzBv3jw6dOhwzvP079+fhx56CLAHptdff52lS5fSpEmTMqmzoquUPTexsbFcdtll3H///Tz66KM8//zz52w7btw4UlNTHVt8fPwlq8vm6cHIng1Y9Gh3ejeNJL/Q4N1f99D7tWV8vylBQ1UiIuJSu3btnF4XFBTw4osv0rJlS0JDQwkICOCnn34iLi7O5Xlatmzp+PnU8FdSUtIlqbkiqvTdCYZhkJOTc873bTYbNputHCuC2iF+vD+0HYu3Hea5BVs4cOwE//jkT65sFM6E65sTG+ZfrvWIiLg7Xy8Ptk7sa9q1y4q/v/P3w+TJk3n99dd54403uOyyy/D392fMmDHnXQn97InIFoulSi0wamq4ycjIYPfu3Y7Xe/fuZcOGDYSEhBATE8O4ceM4ePAgs2fPBuDtt98mJibG0a22YsUK/u///o/Ro0ebUv/59GoaSZcGYUxdspt3lu3h151H6Pv6r4zoUZ+HetTHpwz/hRARqcosFkuZDQ1VJMuXL+eGG27g7rvvBuyLS+7atYumTZuaXFnFZuo/CX/88Qc9e/Z0vB47dixw+l7/hIQEp663wsJCxo0bx969e/H09KR+/fr85z//4cEHHyz32kvKx8uDsX0ac2ObWjz7zWaW70rmv4t38fX6A0y4vjlXNYk0u0QREamgGjRowJdffsmqVauoXr06r732GomJiQo352FquOnRo4fLeShnP8xo9OjRFbaX5nxiw/yZfW97vt+cyMRvtxJ/9AT3zvqDq5tF8tyAZtSqrlVtRUTE2TPPPMPevXvp27cvfn5+PPDAAwwcOJDU1FSzS6vQLEYVm+WalpZGcHAwqampBAUFmVJDZk4+/128ixkr9pJfaODjZWX0VQ0Z3i0Wm6eGqkREzic7O5u9e/cSGxuLj4+P2eVIGXH1ey3N93elvFuqsvO3eTKuf1MWPtKN9rEhZOcV8uqPO+j35nJW7k42uzwREZFKTeHGRI0iA5n7QEdeH9SKsAAbe45kctf7vzHq0z85nJZtdnkiIiKVksKNySwWCze2rsXif3ZnaKc6WC3w3V8J9Jq8jPeX7yG/oOrcuiciIlIWFG4qiGBfLybc0IIFo7pyee1qZOTk88L/tnHdWytYu++o2eWJiIhUGgo3FUyLmsF89Y/O/Oemy6jm58X2xHRufWc1//x8I8kZ535YoYiIiNgp3FRAVquF29vH8Ms/e3D7FbUB+PLPA1z1f0v5aM1+Cgqr1A1uIiIipaJwU4GF+Hvzn5tb8tVDnWkeHURadj7PzN/MjVNXsjH+uNnliYiIVEgKN5VAm5jqLBjVlQnXNyfQx5O/DqQycOpK/vX1Jo5nuV5fREREpKpRuKkkPKwWhnauyy//7MFNrWtiGPDpb3FcNXkZn/8RT6GGqkRERACFm0onPNDGa4Mu57MHOtIwIoCjmbk88cVf3PruarYeSjO7PBERuYR69OjBmDFjHK/r1q3LG2+84fIYi8XC/PnzL/raZXWe8qBwU0l1rBfKwke68a/+TfDz9mDd/mMMmLKCid9uJT07z+zyRETkLAMGDKB3797Fvrd69WosFgt//vlnqc65du1aHnjggbIoz+H555/n8ssvL7I/ISGBfv36lem1LhWFm0rMy8PKA1fWZ/E/u9P/sigKCg1mrtxLr8nL+GbDQZeLkoqISPm67777+OWXX9i/f3+R92bOnMnll19OmzZtSnXO8PBw/PzKZ+HlqKgobDZbuVzrYincuIEawb5MvastH97bntgwf5LSc3jksw3c9f5v7E5KN7s8EREBrrvuOiIiIpg1a5bT/qysLObOncvAgQO54447qFWrFn5+flx22WXMmTPH5TnPHpbatWsXV155JT4+PjRr1oxFixYVOebJJ5+kUaNG+Pn5Ua9ePZ555hny8uw9/rNmzWLChAls3LgRi8WCxWJx1Hv2sNSmTZu46qqr8PX1JTQ0lAceeICMjAzH+8OGDWPgwIH83//9HzVq1CA0NJSRI0c6rnUpeV7yK0i56d4onB/GdGP6sj1MWbKbVX+n0O/N5dzXtR4P92qAn7d+3SLipgwD8rLMubaXH1gs523m6enJkCFDmDVrFs8++yyWk8fMmzeP3Nxchg8fzpw5c3jyyScJCgrif//7H4MHD6ZevXp06NDhvOcvLCzkpptuIiwsjDVr1pCWluY0P+eUwMBAZs2aRXR0NJs2beL+++8nMDCQJ554gkGDBrF582Z++OEHfv75ZwCCg4OLnCMrK4trrrmGjh07snbtWpKSkhg+fDijRo1yCm9LliyhRo0aLFmyhN27dzNo0CAuv/xy7r///vN+nouhbzs3Y/P0YHSvhgxsXZPnF2xh8fYk3ln2N99uPMQz1zWjb/NIx79QIiJuIy8LXoo259r/OgTe/iVqeu+99/Lqq6+ydOlSevbsCdiHpG666SZq1qzJY4895mg7evRofvjhB+bNm1eicPPzzz+zbds29u3bR61atQB46aWXisyTefrppx0/161bl3/+85/MnTuXJ554Al9fXwICAvD09CQqKuqc1/rkk084ceIEs2fPxt/f/tmnTJnCgAEDePnll4mMjASgevXqTJkyBQ8PD5o0acK1117L4sWLL3m40bCUm6od4seMYVfw3pB21Kzmy8HjJxjx8TrumbWW/SmZZpcnIlIlNWnShM6dOzNz5kwA/v77b5YvX869995LQUEBL774Ii1btiQ0NJSAgAB++ukn4uLiSnTubdu2ERMT4wg2AJ06dSrS7osvvqBr165ERUUREBDAM888U+JrnHmtVq1aOYINQJcuXSgsLGTHjh2Ofc2bN8fDw8PxukaNGiQlJZXqWhdCPTdu7upmkXRtEMaUJbuY/uselu44wtWv/8pDPeozont9fLw8zn8SEZGKzsvP3oNi1rVL4b777mPUqFG8/fbbfPDBB9SpU4devXrx6quv8vrrr/PGG29w2WWX4e/vz5gxY8jNLdnDWou7ieTsnvo1a9Zw++23M2HCBPr27UtwcDCfffYZkydPLtVnMAzjnKMAZ+738vIq8l5hYWGprnUh1HNTBfh6e/B43yb8MOZKujYIIze/kDd+3kXfN35lyY5Ln6BFRC45i8U+NGTGVsqh/ttuuw0PDw8+/fRTPvzwQ+655x4sFgvLly/nhhtu4O6776ZVq1bUq1ePXbt2lfi8zZo1Iy4ujkOHToe81atXO7VZuXIlderUYfz48bRr146GDRsWuXvL29ubgoKC815rw4YNZGaeHglYuXIlVquVRo0albjmS0XhpgqpHx7AR/e1Z8qdrYkMsrE/JYt7PljLgx/9wcHjJ8wuT0SkSggICGDQoEH861//4tChQwwbNgyABg0asGjRIlatWsW2bdt48MEHSUxMLPF5e/fuTePGjRkyZAgbN25k+fLljB8/3qlNgwYNiIuL47PPPuPvv//mv//9L19//bVTm7p167J37142bNhAcnIyOTk5Ra5111134ePjw9ChQ9m8eTNLlixh9OjRDB482DHfxkwKN1WMxWLhupbRLP5nD4Z3jcXDauHHLYfpPXkZ05b+TW7+pe8uFBGp6u677z6OHTtG7969iYmJAeCZZ56hTZs29O3blx49ehAVFcXAgQNLfE6r1crXX39NTk4O7du3Z/jw4bz44otObW644QYeffRRRo0axeWXX86qVat45plnnNrcfPPNXHPNNfTs2ZPw8PBib0f38/Pjxx9/5OjRo1xxxRXccsst9OrViylTppT+L+MSsBhV7ElvaWlpBAcHk5qaSlBQkNnlmG57YhrPzN/M2n3HAGgQEcDEG5rTuX6YyZWJiJxbdnY2e/fuJTY2Fh8fH7PLkTLi6vdamu9v9dxUcU2igvj8wU5MvrUVYQHe7E7K4M73fuPRuRvIyMk3uzwREZFSU7gRLBYLN7etxeJ/9mBIpzpYLPD1+oNcP2UFuw7rCcciIlK5KNyIQ7CvFxNvaMEXIzpTI9iHPUcyueHtlSzYaNLtlSIiIhdA4UaKaFunOt+N7krn+qFk5Rbw8Jz1PL9giyYbi4hIpaBwI8UKDbDx0X0deKhHfQBmrdrHHe+tITE12+TKREROq2L3xLi9svp9KtzIOXlYLTxxTROmD25LoI8n6/Yf47q3lrPq72SzSxORKu7UI/1L+vReqRxO/T7PXLLhQmj5BTmvPs2j+DYykBEfr2N7Yjp3v/8bT1zThAevrKdFOEXEFJ6envj5+XHkyBG8vLywWvX/6pVdYWEhR44cwc/PD0/Pi4snes6NlNiJ3ALGz9/EV38eBKBPs0j+77ZWBPl4nedIEZGyl5uby969e8tlrSIpH1arldjYWLy9vYu8V5rvb4UbKRXDMPj09zgmLNhKbkEhsWH+TLu7DU2i9HcpIuWvsLBQQ1NuxNvb+5y9cAo3LijclI2N8cf5x8frOJSajY+XlUk3XcaNrWuZXZaIiLgpPaFYLrlWtavx3cPd6NYwjOy8Qh6du5Fn5m8mJ9/1SrIiIiKXmsKNXLAQf29m3dOeh69qAMBHa/Yz6N01HNIK4yIiYiJTw82vv/7KgAEDiI6OxmKxMH/+fJftv/rqK66++mrCw8MJCgqiU6dO/Pjjj+VTrBTLw2phbJ/GzBzWjiAfTzbEH+e6t1awcrduFxcREXOYGm4yMzNp1apViZdI//XXX7n66qtZuHAh69ato2fPngwYMID169df4krlfK5qEsn/Hu5G8+ggjmbmMnjGb7y9ZDeFhVVqSpeIiFQAFWZCscVi4euvv2bgwIGlOq558+YMGjSIZ599tkTtNaH40srOK+DZbzbz+R8HAOjdNILJt11OsK9uFxcRkQtXZSYUFxYWkp6eTkhIyDnb5OTkkJaW5rTJpePj5cErt7Ti5Zsvw9vTys/bkhjw1gq2HEo1uzQREakiKnW4mTx5MpmZmdx2223nbDNp0iSCg4MdW+3atcuxwqpr0BUxfDmiM7Wq+xJ3NIubpq7ii3UHzC5LRESqgEobbubMmcPzzz/P3LlziYiIOGe7cePGkZqa6tji4+PLscqq7bJawXw3uis9GoeTk1/IY/M2Mu6rTbpdXERELqlKGW7mzp3Lfffdx+eff07v3r1dtrXZbAQFBTltUn6q+Xkzc+gVPNq7ERYLzPk9jlvfWc2BY1lmlyYiIm6q0oWbOXPmMGzYMD799FOuvfZas8uRErBaLTzSuyEfDLuCan5e/HUgleveWsGynUfMLk1ERNyQqeEmIyODDRs2sGHDBgD27t3Lhg0biIuLA+xDSkOGDHG0nzNnDkOGDGHy5Ml07NiRxMREEhMTSU3VZNXKoEfjCL4b3ZWWtYI5npXHsA9+582fd+l2cRERKVOmhps//viD1q1b07p1awDGjh1L69atHbd1JyQkOIIOwLvvvkt+fj4jR46kRo0aju2RRx4xpX4pvVrV/fj8wU7c0T4Gw4DXf97JvR+u5XiWFr4TEZGyUWGec1Ne9JybimPeH/E8PX8zOfmF1Kruyzt3t6VFzWCzyxIRkQqoyjznRiq3W9vV5quHOhMT4seBYye4adoq5q6NO/+BIiIiLijciKmaRwfz7aiu9GoSQW5+IU9+uYknv/iL7DzdLi4iIhdG4UZMF+znxXtD2vF438ZYLTD3j3hueWcV8Ud1u7iIiJSewo1UCFarhZE9GzD73g6E+Huz+WAa1721giXbk8wuTUREKhmFG6lQujYM47vRXWlVuxqpJ/K4Z9ZaXvtpBwW6XVxEREpI4UYqnOhqvnz+YEcGd6wDwH9/2c2wD37naKZuFxcRkfNTuJEKyebpwb8HtuD1Qa3w8bKyfFcyA95awcb442aXJiIiFZzCjVRoN7auxfyRXagb6sfB4ye49Z3VfPpbHFXs8UwiIlIKCjdS4TWJCmLB6K70aRZJbkEh//p6E4/N+4sTubpdXEREilK4kUohyMeLdwe35al+TbBa4Ms/D3DTtFXsT8k0uzQREalgFG6k0rBYLIzoXp+Ph3cgLMCbbQn228V/3nrY7NJERKQCUbiRSqdz/TC+G92NNjHVSM/OZ/jsP3j1x+26XVxERACFG6mkooJ9+OyBTgzrXBeAt5f8zZCZv5GSkWNuYSIiYjqFG6m0vD2tPH99c/57R2v8vD1YuTuF695awfq4Y2aXJiIiJlK4kUrv+lbRzB/ZhXrh/iSkZnPbu6v5aPU+3S4uIlJFKdyIW2gUGcg3I7vQr0UUeQUGz3yzhbGfbyQrN9/s0kREpJwp3IjbCPTxYupdbRjfvykeVgtfrz/IdW+t4PM/4snO0zNxRESqCotRxfru09LSCA4OJjU1laCgILPLkUvktz0pjJqzniPp9gnG1f28GHRFDHd3jKFWdT+TqxMRkdIqzfe3wo24reNZuXy2Np6PVu/n4PETAFgt0LtpJMM616VT/VAsFovJVYqISEko3LigcFP1FBQaLN52mA9X72Pl7hTH/oYRAQzpXJebWtfE3+ZpYoUiInI+CjcuKNxUbbuT0pm9ej9frjtA5sm1qQJtntzcthZDOtWhXniAyRWKiEhxFG5cULgRgPTsPL5cd4DZq/ezJ/n0+lRXNgpnWOc69GgUgdWqISsRkYpC4cYFhRs5U2GhwYrdyXy4ah+/7Eji1L8NMSF+DOlUh1vb1ibYz8vcIkVEROHGFYUbOZe4lCw+/m0/c9fGk3oiDwAfLys3tq7JkE51aVpD/7yIiJhF4cYFhRs5nxO5BXyz4SCzVu1je2K6Y3/72BCGda7L1c0i8fLQI6JERMqTwo0LCjdSUoZhsHbfMT5ctY8ftiQ6Vh2PCvLh7o4x3N4+hrAAm8lViohUDQo3LijcyIVITM3m09/28+nvcSRn5ALg7WHl2pY1GNq5LpfXrmZugSIibk7hxgWFG7kYOfkFfL8pkVmr9rEh/rhjf6tawQztXJdrW9bA5ulhXoEiIm5K4cYFhRspKxvjj/Ph6n18tzGB3IJCAEL9vbm9fW3u7liHGsG+JlcoIuI+FG5cULiRspaSkcNna+P5eM1+ElKzAfCwWujTLJKhnevSITZEyzyIiFwkhRsXFG7kUskvKOTnbYeZtWofa/YcdexvEhXIkE51Gdg6Gj9vLfMgInIhFG5cULiR8rAjMZ0PV+/j6z8PciLPvsxDkI8nt7WrzeBOdagT6m9yhSIilYvCjQsKN1KeUk/kMe+PeD5as5/9KVkAWCzQs3EEQzrV4cqG4VrmQUSkBErz/W3qk8h+/fVXBgwYQHR0NBaLhfnz57tsn5CQwJ133knjxo2xWq2MGTOmXOoUuVDBvl4M71aPJf/swQfDrqBH43AMA37ZnsSwD9bS67VlzFyxl7TsPLNLFRFxG6aGm8zMTFq1asWUKVNK1D4nJ4fw8HDGjx9Pq1atLnF1ImXHarXQs0kEs+5pz5LHenBvl1gCfTzZm5zJxO+20vGlxTw9fxM7D6ef/2QiIuJShRmWslgsfP311wwcOLBE7Xv06MHll1/OG2+8UarraFhKKorMnHy+Xn+Q2av3sfNwhmN/p3qhDO1cl95NI/DUMg8iIkDpvr9164aISfxtntzdsQ53dYhh9Z4UZq/az09bE1m9J4XVe1KoWc2XuzrGcPsVMYT4e5tdrohIpeH24SYnJ4ecnBzH67S0NBOrESnKYrHQuX4YneuHcfD4CT5Zs5/P1sZz8PgJXvlhB2/8vIt+LaJoE1OdZtFBNIkKJNDHy+yyRUQqLLcPN5MmTWLChAlmlyFSIjWr+fLENU14uFdDvvsrgQ9X7WPTwVS+2XCIbzYccrSrG+pHs+ggmtUIOvlnMJFBNj0sUESEKhBuxo0bx9ixYx2v09LSqF27tokViZyfj5cHt7Stxc1tarI+/ji/bEtiW0IaWxPSSEjNZl9KFvtSsli4KdFxTKi/91mBJ4h64QF46FZzEali3D7c2Gw2bDab2WWIXBCLxUKbmOq0ianu2JeSkcO2hHS2JqSy9ZA98OxOyiAlM5flu5JZvivZ0dbHy0rjKOfA07RGoJ6ULCJuzdT/wmVkZLB7927H671797JhwwZCQkKIiYlh3LhxHDx4kNmzZzvabNiwwXHskSNH2LBhA97e3jRr1qy8yxcxRWiAja4NbXRtGObYl51XwI7EdLYmpDkCz7aENLJyC9gYf5yNZ6xgbrFAbJi/U+BpFh1ERKCPCZ9GRKTsmXor+NKlS+nZs2eR/UOHDmXWrFkMGzaMffv2sXTpUsd7xc0pqFOnDvv27SvRNXUruFQVhYUG+1IynQLP1kNpJKXnFNs+PNBWJPDUDfXXsJaIVAhafsEFhRup6o6k55wVeFLZk5xJcf8l8PP2oElUoGPS8qm7tXy8PMq/cBGp0hRuXFC4ESkqKzef7YnpTj082xPTyM4rLNLWaoH64QFFJi+HBmhum4hcOgo3LijciJRMQaHB3uQMtpwReLYeSiMlM7fY9lFBPkUCT0yInxYGFZEyoXDjgsKNyIUzDIMj6TnOgSchjb3JmcW2D7B50rRGoNPzeBpFBWDz1LCWiJSOwo0LCjciZS8jJ5/tCc6BZ3tiOrn5RYe1PK0WGkQE0KJmMM2jg2geHUzTGnrqsoi4pnDjgsKNSPnILyhkT3ImWw6dfh7PlkNpHM/KK7Z93VA/mp8ReJpHBxGmeTwicpLCjQsKNyLmMQyDQ6nZbDmYyuZD9ju1thyyP3W5OFFBPifDThDNTgaeWtV9tcyESBWkcOOCwo1IxZOSYb89ffPBNEdPz96U4m9PD/b1cgSe5tHBtKgZRGyYlpkQcXcKNy4o3IhUDqfm8Ww+aO/d2XIojV1J6eQVFP1Plq+XB01qBJ4OPNGauCzibhRuXFC4Eam8cvIL2HU4gy2HTgeeU8tMnO3UxOVT83da1NTEZZHKTOHGBYUbEfdifx7P6YnL9tCTyjFXE5ejg2leUxOXRSoThRsXFG5E3N+ZE5dP9fBsPZTKoXNMXI4Msp0cztLEZZGKSuHGBYUbkarraGau05DWloOpJZ643Dw6iHrhmrgsYhaFGxcUbkTkTJk5+WxLOD2ctfnguScu+3hZaVojiBbRwXRtGEa3hmH4eXuaULVI1aNw44LCjYicT25+ITsP2xcS3Xyyp6e4icvenlY61w+lV9NIejWJILqar0kVi7g/hRsXFG5E5EIUFBrsS8lky6E01u07yuLtSRw4dsKpTbMaQfRqGkGvppG0rBmsRUNFypDCjQsKNyJSFgzDYFdSBj9vO8zibUn8GXfMae5OeKCNqxpHcFXTCA1fiZQBhRsXFG5E5FJIychh6Y4jLN5+mF93JpORk+94T8NXIhdP4cYFhRsRudRy8wv5bW8Ki7cl8fO2wxq+EikDCjcuKNyISHnS8JVI2VC4cUHhRkTMdDQzlyXbkzR8JVJKCjcuKNyISEVx5vDV4u2HiT/qPHzVtEYQvTV8JQIo3LikcCMiFdGp4avF25JYvO0wf8Ydo1DDVyIOCjcuKNyISGWg4SsRZwo3LijciEhlk5tfyO97j9onJWv4SqoohRsXFG5EpDIzDIPdSRn8rOErqWIUblxQuBERd3Jq+OqX7Uks23lEw1fithRuXFC4ERF3peErcWcKNy4o3IhIVVDS4aueTcLp0iCMQB8v84oVKQGFGxcUbkSkKjqamcvSHUks3lZ0+MrTaqFd3er0bBxBj8YRNIoMwGJRr45ULAo3LijciEhVl5tfyNp99uGrZTuOsCc50+n96GAfujeOoGdje6+Ov02TksV8CjcuKNyIiDjbn5LJ0h1HWLojiVV/p5CTX+h4z8vDQvvYEHo0sg9h1Q9Xr46YQ+HGBYUbEZFzy84rYM2eFJbuOMKSHUnsT8lyer9mNV96NgmnR6MIOjcI1a3mUm4UblxQuBERKbm9yZks2Z7E0p1HWLMnhdwzenW8Pax0qBdCj8YR9GgcTr0wf/XqyCWjcOOCwo2IyIU5kVvA6j3Jjl6ds281jwnxo0fjcHo0DqdTvTB8vT1MqlTcUaUJN7/++iuvvvoq69atIyEhga+//pqBAwe6PGbZsmWMHTuWLVu2EB0dzRNPPMGIESNKfE2FGxGRi2cYBntO9uos23mE3/YcJbfgjF4dTyud6oXSo3E4PRtHUDfM38RqxR2U5vvb1MHSzMxMWrVqxT333MPNN9983vZ79+6lf//+3H///Xz88cesXLmShx56iPDw8BIdLyIiZcNisVA/PID64QEM71aPzJx8Vv2dwtIdSSzdcYSDx0+wbOcRlu08woRvt1I31M8xfNWxXig+XurVkUunwgxLWSyW8/bcPPnkkyxYsIBt27Y59o0YMYKNGzeyevXqEl1HPTciIpfWqQcILjkZdNbuO0pewemvGh+vU706EfRsHEFMqJ+J1UplUWl6bkpr9erV9OnTx2lf3759mTFjBnl5eXh5FX3CZk5ODjk5OY7XaWlpl7xOEZGqzGKx0DAykIaRgTxwZX0ycvJZuTvZ0auTkJrNkh1HWLLjCM+xhXph/o5enfaxIerVkYtWqcJNYmIikZGRTvsiIyPJz88nOTmZGjVqFDlm0qRJTJgwobxKFBGRswTYPOnbPIq+zaMwDIMdh9Mdz9X5Y98x9iRnsid5LzNX7sXXy4PO9UPp0SSCHo3CqR2iXh0pvUoVboAitxmeGlU71+2H48aNY+zYsY7XaWlp1K5d+9IVKCIi52SxWGgSFUSTqCBGdK9PWnYeq3Yns2S7/Q6spPQcFm9PYvH2JAAaRATQo1E4PZtE0K5udWye6tWR86tU4SYqKorExESnfUlJSXh6ehIaGlrsMTabDZvNVh7liYhIKQX5eHFNixpc06IGhmGwLSGdJTuSWLbjCOvijrE7KYPdSRm8v2Ivft4edGkQdvJ28whqVvM1u3ypoCpVuOnUqRPffvut076ffvqJdu3aFTvfRkREKg+LxUKz6CCaRQcxsmcDUk/ksWJXsj3s7DzCkfQcFm09zKKthwFoFBlAz8YRdG8cTsta1QjQGlhykql3S2VkZLB7924AWrduzWuvvUbPnj0JCQkhJiaGcePGcfDgQWbPng3YbwVv0aIFDz74IPfffz+rV69mxIgRzJkzp8S3gutuKRGRyqew0GBrQhpLdySxZMcR1scdo/Csb68awT40iLDfnt4g4vQW6u+tJye7gUrzEL+lS5fSs2fPIvuHDh3KrFmzGDZsGPv27WPp0qWO95YtW8ajjz7qeIjfk08+qYf4iYhUMcezcvl1l/0OrBW7kklKzzln22p+XjQ4I/DUjwigQXgANav5YrUq9FQWlSbcmEHhRkTE/RzPyuXvIxmOOTq7kzLYfSSDA8dOcK5vOV8vD+qF+9tDzxnhp06oP96e1vL9AHJeCjcuKNyIiFQdJ3IL2JNsDzt/nww8u5My2Juc6fRgwTN5Wi3EhPo5BZ5Tw13+mtdjGoUbFxRuREQkv6CQuKNZjh4eR/hJyiAzt+Ccx0UH+9iHtU5tJwNQaIDuyr3UFG5cULgREZFzMQyDxLRs5+GtpAz+PpJBckbuOY+r7ufl1MNz6ufoYM3rKSsKNy4o3IiIyIU4npVbZE7P7iT7vJ5z8fXyoH6Ef5Ehrjqh/nh5aF5PaSjcuKBwIyIiZelEbgF/H8koMqF5X4rreT11Qv2cAk+D8EBqVvelup+Xbl0vhsKNCwo3IiJSHvLOnNdzxoTmv88zr8fbw0p4oI3wQBsRgTYig3yICLQREWQjItDH8Weov3eVGvJSuHFB4UZERMxkGAYJqdlFhrf2nGdez9k8rBbCAryJCPQhMshGeODpEBR5RggKC/DG0w2GwBRuXFC4ERGRiionv4DkjFwOp2WTlJbDkfRsktJzSErL4XC6fV9Seg4pmTnnfH7P2SwWCPX3PqPXx+YciE7uCw+0VeiFSUvz/a0b9kVERCoIm6cHNav5nndR0PyCQpIzckk6I/AcTrMHoTMD0ZGMHAoKDZIzcknOyGVrguvrV/fzcoSg8DOHxM4KRr7eFTcEgcKNiIhIpePpYSUq2IeoYB+X7QoKDY5mngxB6TkkpZ0OQ0np2RxOy+HIyZ/zCgyOZeVxLCuPHYfTXZ430MfTqQco4mQICj9jX73wgLL8yKWicCMiIuKmPKwWx+Tk5i7aGYbB8aw8px6g071CzoEoO6+Q9Ox80rPz+ftIZrHnC/TxZNPzfS/NhyoBhRsREZEqzmKxUN3fm+r+3jSOCjxnO8MwSM/JL7YH6FTP0JH0HAJ9zI0XCjciIiJSIhaLhSAfL4J8vGgQce4QZLbKf2+YiIiIyBkUbkRERMStXFC4iY+P58CBA47Xv//+O2PGjGH69OllVpiIiIjIhbigcHPnnXeyZMkSABITE7n66qv5/fff+de//sXEiRPLtEARERGR0rigcLN582bat28PwOeff06LFi1YtWoVn376KbNmzSrL+kRERERK5YLCTV5eHjabDYCff/6Z66+/HoAmTZqQkHCexx+KiIiIXEIXFG6aN2/OO++8w/Lly1m0aBHXXHMNAIcOHSI0NLRMCxQREREpjQsKNy+//DLvvvsuPXr04I477qBVq1YALFiwwDFcJSIiImKGC14VvKCggLS0NKpXr+7Yt2/fPvz8/IiIiCizAsuaVgUXERGpfErz/X1BPTcnTpwgJyfHEWz279/PG2+8wY4dOyp0sBERERH3d0Hh5oYbbmD27NkAHD9+nA4dOjB58mQGDhzItGnTyrRAERERkdK4oHDz559/0q1bNwC++OILIiMj2b9/P7Nnz+a///1vmRYoIiIiUhoXFG6ysrIIDLQvmPXTTz9x0003YbVa6dixI/v37y/TAkVERERK44LCTYMGDZg/fz7x8fH8+OOP9OnTB4CkpCRN0hURERFTXVC4efbZZ3nssceoW7cu7du3p1OnToC9F6d169ZlWqCIiIhIaVzwreCJiYkkJCTQqlUrrFZ7Rvr9998JCgqiSZMmZVpkWdKt4CIiIpVPab6/PS/0IlFRUURFRXHgwAEsFgs1a9bUA/xERETEdBc0LFVYWMjEiRMJDg6mTp06xMTEUK1aNf79739TWFhY1jWKiIiIlNgF9dyMHz+eGTNm8J///IcuXbpgGAYrV67k+eefJzs7mxdffLGs6xQREREpkQuacxMdHc0777zjWA38lG+++YaHHnqIgwcPllmBZU1zbkRERCqfS778wtGjR4udNNykSROOHj1aqnNNnTqV2NhYfHx8aNu2LcuXL3fZ/u2336Zp06b4+vrSuHFjx5OSRUREROACw02rVq2YMmVKkf1TpkyhZcuWJT7P3LlzGTNmDOPHj2f9+vV069aNfv36ERcXV2z7adOmMW7cOJ5//nm2bNnChAkTGDlyJN9+++2FfAwRERFxQxc0LLVs2TKuvfZaYmJi6NSpExaLhVWrVhEfH8/ChQsdSzOcT4cOHWjTpo3TelRNmzZl4MCBTJo0qUj7zp0706VLF1599VXHvjFjxvDHH3+wYsWKEl1Tw1IiIiKVzyUflurevTs7d+7kxhtv5Pjx4xw9epSbbrqJLVu28MEHH5ToHLm5uaxbt87xdONT+vTpw6pVq4o9JicnBx8fH6d9vr6+/P777+Tl5V3IRxERERE3c8HPuYmOji5yV9TGjRv58MMPmTlz5nmPT05OpqCggMjISKf9kZGRJCYmFntM3759ef/99xk4cCBt2rRh3bp1zJw5k7y8PJKTk6lRo0aRY3JycsjJyXG8TktLK8nHExERkUrqgnpuypLFYnF6bRhGkX2nPPPMM/Tr14+OHTvi5eXFDTfcwLBhwwDw8PAo9phJkyYRHBzs2GrXrl2m9YuIiEjFYlq4CQsLw8PDo0gvTVJSUpHenFN8fX2ZOXMmWVlZ7Nu3j7i4OOrWrUtgYCBhYWHFHjNu3DhSU1MdW3x8fJl/FhEREak4TAs33t7etG3blkWLFjntX7RoEZ07d3Z5rJeXF7Vq1cLDw4PPPvuM6667zrG+1dlsNhtBQUFOm4iIiLivUs25uemmm1y+f/z48VJdfOzYsQwePJh27drRqVMnpk+fTlxcHCNGjADsvS4HDx50PMtm586d/P7773To0IFjx47x2muvsXnzZj788MNSXVdERETcV6nCTXBw8HnfHzJkSInPN2jQIFJSUpg4cSIJCQm0aNGChQsXUqdOHQASEhKcnnlTUFDA5MmT2bFjB15eXvTs2ZNVq1ZRt27d0nwMERERcWMX9JybykzPuREREal8LvlzbkREREQqKoUbERERcSsKNyIiIuJWFG5ERETErSjciIiIiFtRuBERERG3onAjIiIibkXhRkRERNyKwo2IiIi4FYUbERERcSsKNyIiIuJWFG5ERETErSjciIiIiFtRuBERERG3onAjIiIibkXhRkRERNyKwo2IiIi4FYUbERERcSsKNyIiIuJWFG5ERETErSjciIiIiFtRuBERERG3onAjIiIibkXhRkRERNyKwo2IiIi4FYUbERERcSsKNyIiIuJWFG5ERETErSjciIiIiFtRuBERERG3onAjIiIibkXhRkRERNyKwo2IiIi4FYUbERERcSumh5upU6cSGxuLj48Pbdu2Zfny5S7bf/LJJ7Rq1Qo/Pz9q1KjBPffcQ0pKSjlVKyIiIhWdqeFm7ty5jBkzhvHjx7N+/Xq6detGv379iIuLK7b9ihUrGDJkCPfddx9btmxh3rx5rF27luHDh5dz5SIiIlJRmRpuXnvtNe677z6GDx9O06ZNeeONN6hduzbTpk0rtv2aNWuoW7cuDz/8MLGxsXTt2pUHH3yQP/74o5wrFxERkYrKtHCTm5vLunXr6NOnj9P+Pn36sGrVqmKP6dy5MwcOHGDhwoUYhsHhw4f54osvuPbaa895nZycHNLS0pw2ERERcV+mhZvk5GQKCgqIjIx02h8ZGUliYmKxx3Tu3JlPPvmEQYMG4e3tTVRUFNWqVeOtt94653UmTZpEcHCwY6tdu3aZfg4nh7fAiWOX7vwiIiJyXqZPKLZYLE6vDcMosu+UrVu38vDDD/Pss8+ybt06fvjhB/bu3cuIESPOef5x48aRmprq2OLj48u0fofUA/DRjfBeL0jedWmuISIiIufladaFw8LC8PDwKNJLk5SUVKQ355RJkybRpUsXHn/8cQBatmyJv78/3bp144UXXqBGjRpFjrHZbNhstrL/AGfLSQcPbzj6N7zfC26dBfWvuvTXFRERESem9dx4e3vTtm1bFi1a5LR/0aJFdO7cudhjsrKysFqdS/bw8ADsPT6mimgK9/8CtTtAdip8fAv8Nh3MrktERKSKMXVYauzYsbz//vvMnDmTbdu28eijjxIXF+cYZho3bhxDhgxxtB8wYABfffUV06ZNY8+ePaxcuZKHH36Y9u3bEx0dbdbHOC0gAoZ+C63uAKMAvn8c/jcWCvLMrkxERKTKMG1YCmDQoEGkpKQwceJEEhISaNGiBQsXLqROnToAJCQkOD3zZtiwYaSnpzNlyhT++c9/Uq1aNa666ipefvllsz5CUZ42GDjN3pOz6Dn4Y6Z9Ds5ts8EvxOzqRERE3J7FMH08p3ylpaURHBxMamoqQUFBl/ZiO76HL4dDbgZUj4U750J440t7TRERETdUmu9v0++WcmuN+8F9P0G1GDi2F97vDbt+NrsqERERt6Zwc6lFNof7l0BMZ8hJg09vhdVTNdFYRETkElG4KQ/+YTDkG2h9NxiF8OM4+PZhyM81uzIRERG3o3BTXjy94fop0PclsFjhz9nw0UDI1IrmIiIiZUnhpjxZLNBpJNz5OdiCYP9KeK8nJG0zuzIRERG3oXBjhoZXw32LoHpdOL4f3r8adv5odlUiIiJuQeHGLBFN7BON63aD3HT4dBCs/K8mGouIiFwkhRsz+YXA3V9B22GAAYuegW9GQn6O2ZWJiIhUWgo3ZvP0huvegGtetk803vAJfHg9ZBwxuzIREZFKSeGmIrBYoOMIuOsLsAVD/Bp47ypI3Gx2ZSIiIpWOwk1F0qAX3L8YQupDahzM6APb/2d2VSIiIpWKwk1FE9YQhv8Msd0hLxM+uwuWv6aJxiIiIiWkcFMR+YXA3V/CFcMBAxZPgK9HQF622ZWJiIhUeAo3FZWHF1w7Gfr/H1g84K/P4MPrIP2w2ZWJiIhUaAo3FV37+2HwV+BTDQ6stU80TvjL7KpEREQqLIWbyqBeD7j/FwhtCGkHYGZf2LrA7KpEREQqJIWbyiK0vn2icf2rIC8LPh8My17VRGMREZGzKNxUJr7V4M550GGE/fWSF+DL4ZB3wtSyREREKhKFm8rGwxP6vWx/qrHVEzZ/AR/0h7QEsysTERGpEBRuKqt298Dg+eBbHQ79aZ9ofGi92VWJiIiYTuGmMovtZp9oHNYY0g/BzH6w5WuzqxIRETGVwk1lF1IPhi+CBldD/gmYNwyW/kcTjUVEpMpSuHEHPsFw51zoNMr+eukk+OIeyM0yty4RERETKNy4C6sH9H0Rrn8LrF724akP+kHaIbMrExERKVcKN+6mzRAYugD8QiFhA0zvCQfWmV2ViIhIuVG4cUd1OtsnGkc0g4xEmNUfNn1hdlUiIiLlQuHGXVWvC/f9BI36QX42fHkf/PICFBaaXZmIiMglpXDjzmyBcPsn0OUR++tfX4V5QyA309y6RERELiGFG3dn9YCrJ8LAaeDhDdu+tS+8mXrA7MpEREQuCYWbquLyO2Hod+AfDomb7BON49eaXZWIiEiZU7ipSmI62CcaR7aAzCSYdS1snGt2VSIiImVK4aaqqRYD9/4ITa6Dghz4+gH4+XlNNBYREbehcFMV2QLgto+g2z/tr1e8DnPvgpx0c+sSEREpA6aHm6lTpxIbG4uPjw9t27Zl+fLl52w7bNgwLBZLka158+blWLGbsFqh17Nw03vgYYMdC2FGXzgeZ3ZlIiIiF8XUcDN37lzGjBnD+PHjWb9+Pd26daNfv37ExRX/Bfvmm2+SkJDg2OLj4wkJCeHWW28t58rdSMvb4J6FEBAJSVvsE43j1phdlYiIyAWzGIZ5y0d36NCBNm3aMG3aNMe+pk2bMnDgQCZNmnTe4+fPn89NN93E3r17qVOnTomumZaWRnBwMKmpqQQFBV1w7W4n9QDMuQMS/7KvTXXda9B6MFgsZlcmIiJSqu9v03pucnNzWbduHX369HHa36dPH1atWlWic8yYMYPevXuXONiIC8G14N4foOn1UJgHC0bDRzdC8m6zKxMRESkV08JNcnIyBQUFREZGOu2PjIwkMTHxvMcnJCTw/fffM3z4cJftcnJySEtLc9rkHLz94dYP7XNxPGywZwlM6wS/vAh5J8yuTkREpERMn1BsOWvYwzCMIvuKM2vWLKpVq8bAgQNdtps0aRLBwcGOrXbt2hdTrvuzWu13UY1cAw16Q0Eu/PoKvN0Bdv5kdnUiIiLnZVq4CQsLw8PDo0gvTVJSUpHenLMZhsHMmTMZPHgw3t7eLtuOGzeO1NRUxxYfH3/RtVcJIfXgri/st4wH1YTj++HTW+Gzu+C4/g5FRKTiMi3ceHt707ZtWxYtWuS0f9GiRXTu3NnlscuWLWP37t3cd999572OzWYjKCjIaZMSslig2fUw8nfo/DBYPWH7d/B2e1jxBuTnml2hiIhIEaYOS40dO5b333+fmTNnsm3bNh599FHi4uIYMWIEYO91GTJkSJHjZsyYQYcOHWjRokV5l1w12QKgz7/hweUQ0xnysuDn5+DdbrBvhdnViYiIOPE08+KDBg0iJSWFiRMnkpCQQIsWLVi4cKHj7qeEhIQiz7xJTU3lyy+/5M033zSj5Kotspn9mTgbP4OfnoYj2+3rU7W83R5+AiLMrlBERMTc59yYQc+5KSMnjsHif8MfMwEDbMHQ6xlody9YPcyuTkRE3EyleM6NVHK+1e0P+rt/MdS4HHJSYeFj8N5VcHCd2dWJiEgVpnAjF6dmW7j/F+j/f/bem4QN8F4v+G6svXdHRESknCncyMWzekD7+2H0H/b5Nxjwxwx4qx1smANVa+RTRERMpnAjZScgAm56F4Z+B2GNISsZ5o+AD/rD4a1mVyciIlWEwo2UvdhuMGIF9J4AXn4Qt8p+2/hPz0BOhtnViYiIm1O4kUvD0xu6jrE/ALDJdVCYD6v+a38A4NYFGqoSEZFLRuFGLq1qteH2T+DOz6FaHUg7CJ8Phk9uhaN7zK5ORETckMKNlI9GfeGhNXDl4+DhDbsXwdsdYenLkJdtdnUiIuJGFG6k/Hj7wVVPwz9WQb0eUJADS1+CaZ1g92KzqxMRETehcCPlL6whDJ4Pt8yEgCj78NTHN8HnQyHtkNnViYhIJadwI+awWKDFzTBqLXR8CCxW2DofplwBq6ZAQZ7ZFYqISCWlcCPm8gmCaybBA8ugVnvIzYCfxsO73SFujdnViYhIJaRwIxVDjZZw749w/Vv2dauStsDMvjB/JGQmm12diIhUIgo3UnFYrdBmCIxaZ/8TYMPH8FZb+OMDKCw0tz4REakUFG6k4vEPtffg3LcIIi+D7OPw3RiYcTUkbDS7OhERqeAUbqTiqt0eHlgK1/wHvAPh4B8wvQcsfAKyU82uTkREKiiFG6nYPDyh4z/sd1W1uBmMQvj9XftdVX/N0zIOIiJShMKNVA5BNezPxRk8H0IbQMZh+Go4zL4ejuw0uzoREalAFG6kcqnf0/6E46ueBk8f2PsrTOsMiydCbpbZ1YmISAWgcCOVj6fNvkbVyN+gYV8ozIPlk+HtDrDje7OrExERkyncSOVVvS7cORdu/xSCa0NqHMy5HebcAcf2m12diIiYROFGKjeLBZpca+/F6fooWD1hx0J7L87yyZCfa3aFIiJSziyGUbVuN0lLSyM4OJjU1FSCgoLMLkfKWtJ2WPgY7Ftufx1cGxr0hrpd7VtglLn1iYjIBSnN97fCjbgfw4C/PrevUZV5xPm90AYng043qNPFfheWiIhUeAo3LijcVCE5Gfa7qfatsPfkJG4CzvrHXWFHRKRSULhxQeGmCjtxDPavVtgREamEFG5cULgRhxPHIG7N6bCT8BfFhp06Xexhp25XhR0REZMo3LigcCPndOI4xK12HXZC6p/u2anbBYKizahURKTKUbhxQeFGSkxhR0SkwlC4cUHhRi7YieMnh7GW2wNP4l/2hTzPFFLfHnIcw1gKOyIiZUHhxgWFGykzJQo79c7o2VHYERG5UAo3LijcyCVzZtjZvxISNroOO3W6QHBNU0oVEalsFG5cULiRcpOd6tyzo7AjInLBFG5cULgR05Qk7FSPdR7GUtgREQFK9/1t+sKZU6dOJTY2Fh8fH9q2bcvy5ctdts/JyWH8+PHUqVMHm81G/fr1mTlzZjlVK3IRfIKhUV/o8wI8sBSe3Ad3zoPOD0N0G7BY4dheWP8RfP0AvN4M3rwcvhkFf34Ee5ZC8i7IzTT3c4iIVHCeZl587ty5jBkzhqlTp9KlSxfeffdd+vXrx9atW4mJiSn2mNtuu43Dhw8zY8YMGjRoQFJSEvn5+eVcuUgZ8AmGRn3sG5zs2fntjJ6dDfawcyrwnH1sUE37BOWgaOefA0/+6RNsXzVdRKSKMXVYqkOHDrRp04Zp06Y59jVt2pSBAwcyadKkIu1/+OEHbr/9dvbs2UNISMgFXVPDUlJpZKedHsZK3ARph+xbbnrJjvfyLz78OH6uCX4hCkAiUimU5vvbtJ6b3Nxc1q1bx1NPPeW0v0+fPqxatarYYxYsWEC7du145ZVX+Oijj/D39+f666/n3//+N76+vsUek5OTQ05OjuN1Wlpa2X0IkUvJJ8i5Z+eU7DRIT4C0g6cDz9k/nzgGeZmQssu+nYuHzb6kxLl6gYJqgn84WD0u7WcVESlDpoWb5ORkCgoKiIyMdNofGRlJYmJiscfs2bOHFStW4OPjw9dff01ycjIPPfQQR48ePee8m0mTJjFhwoQyr1/END5B9i288bnb5GadFYBO/XnGvswkKMiBY/vs27lYPSGwRtFhrzODUGAUeHiV9ScVEbkgps65AbCc1SVuGEaRfacUFhZisVj45JNPCA4OBuC1117jlltu4e233y6292bcuHGMHTvW8TotLY3atWuX4ScQqYC8/SC0vn07l/wcSE8svufn1M8ZiVCYD6nx9u2cLBAQWfzQV1CN06HIy6fMP6qIyNlMCzdhYWF4eHgU6aVJSkoq0ptzSo0aNahZs6Yj2IB9jo5hGBw4cICGDRsWOcZms2Gz2cq2eBF34GmD6nXs27kU5Nt7eJxCz9lBKAEK8+xBKCMRDv157vP5hUG1GPs1q8VAtZPXr1YXqtW21yQicpFMCzfe3t60bduWRYsWceONNzr2L1q0iBtuuKHYY7p06cK8efPIyMggICAAgJ07d2K1WqlVq1a51C1SpXh4nu6NoV3xbQoLISu5+NBzZiDKz7a3y0o+RwCy2Ie/HOGnjvOfgdH2ekREzsPUu6Xmzp3L4MGDeeedd+jUqRPTp0/nvffeY8uWLdSpU4dx48Zx8OBBZs+eDUBGRgZNmzalY8eOTJgwgeTkZIYPH0737t157733SnRN3S0lYgLDsE9yTj0Ax/fD8Tg4tt/+86k/87Jcn8PqaR/mcgo+dU+HoYBI3fkl4sYqxd1SAIMGDSIlJYWJEyeSkJBAixYtWLhwIXXq2LvJExISiIuLc7QPCAhg0aJFjB49mnbt2hEaGsptt93GCy+8YNZHEJGSsFjst537hUCNlkXfNwzITLaHnuP7zgo+cfb5PgW5J4PR/uKv4elz1lBXjHPvj291hR+RKkLLL4hIxVdYaL/768yenjN7f9IOFl3K4my2IHvIKW7Yq1oM2ALK57OIyAWpND03IiIlYrXa19kKrgl1Ohd9Pz8X0g4UP9x1PA4yDkNOGhzeZN+K4xdadJ5PtVObJjuLVCYKNyJS+Xl621dYD6lX/Pu5WfahLUfw2XdyCOxkCMo+Dlkp9s3VZOczg09wrZNbbftcIG+/S/gBRaQ0FG5ExP15+9kfeniuBx9mp56e33Nmr8+Zk53TD9m3uNXFn8M35HTYCa7pHH6Ca9knPOtJzyLlQuFGRMQn2D7R+VyTnbNSTgadfWdMcj5gn+tzPN6+3teJo/Yt8a/ir2H1tN/O7gg9tU6GoNqnX/sEF3+siJSKwo2IiCsWC/iH2bdabYtvk51qDzvn2tIPnXzSc5x9OxdbkH2IyykAndETFBhtH4ITEZcUbkRELpZPsH2LbF78+4UF9knNqQdOLmVxKvgcPP36xFH7pOcjaXBk2zkuZLGv4xVc64wQVNu5F8gvVLe8S5WncCMicqlZPU4/6bl2++Lb5Gbaw07a2T0/8SdD0AH7QqfpCfaNtcWfx9PndK9PUK3ie4G8iq7DJ+JOFG5ERCoCb38Ib2TfinPqQYenenrSDhbtCco4bF/mImW3fTsXv9DT4Scg3L7ml3/YyT9D7X/6hdr36RZ4qYQUbkREKgOLxR5EAsKhZpvi2+Tn2NfyOrPn5+yeoNyM07e9J2w8/3W9A08HnrMDUHGvvf3L9nOLXACFGxERd+Fpg5BY+1Ycw7A/0+fUMFfaAXtvUObJBU0zk+2h59SfRoH9TrDcdPuzgUpUg+/J0BN6Rvhx8doWpDlCUuYUbkREqgqLxb7Glm91iGrhum1h4emHGzqFn2TITCn+dUEu5J84OVQWX7KarF6nw855A1GYvXar9aL/KsS9KdyIiEhRVuvpxU7DGp6/vWFATvrpIa+SBKK8TCjMO2OSdAlYrPYHJjoNiYWCx6lb5E/2Ajl6g85+7eq9Yl6Xpq3j9bneo5i2Z13HJxj8w503W6B6t0pJ4UZERC6exQI+QfbtXMNiZ8s74SL8JJ8VklIgJ9W+QGrWyX1VhYftZNAJs/8ZEHH65zP3+5/c7+FldsWmU7gRERFzePnaFyWtVrtk7fNzT/YMnWN+kGGcbHjyzzNfn/M9irZ1vC7mPOd7Xaq2nPVeof2BkJlH7FvGEXvvVkGOfX5U2oFz/tU48anmHHwCIs4KQWdsPsFu2SukcCMiIpWDpzcE1bBvVUVu5ulJ35lHIDPp5J/Jp0NQZjJkJNlDn3FyrlT2cUjZdf7zW73O0yt06r2ISvVoAIUbERGRisrb375Vr3P+toWFcOLYGaHn7C3ZuVcoN/3knKeTi8KWhC34jPBzZq9QMcNjfiEX99kvgsKNiIiIO7Ba7ROs/UOBJudvf2rOkyP4FNcrdDIIZSXb10fLSbVvR/92fW5bMIxzsY7aJaZwIyIiUhWVZs7TqUcDnB18zu4Zykiy/+kfdsnLd0XhRkRERFw789EA51oi5EwF+Ze+Jhf0JCQREREpWx7m9p0o3IiIiIhbUbgRERERt6JwIyIiIm5F4UZERETcisKNiIiIuBWFGxEREXErCjciIiLiVhRuRERExK0o3IiIiIhbUbgRERERt6JwIyIiIm5F4UZERETcisKNiIiIuBVzl+00gWEYAKSlpZlciYiIiJTUqe/tU9/jrlS5cJOeng5A7dq1Ta5ERERESis9PZ3g4GCXbSxGSSKQGyksLOTQoUMEBgZisVjK9NxpaWnUrl2b+Ph4goKCyvTcUnr6fVQs+n1UPPqdVCz6fbhmGAbp6elER0djtbqeVVPlem6sViu1atW6pNcICgrSP5gViH4fFYt+HxWPficVi34f53a+HptTNKFYRERE3IrCjYiIiLgVhZsyZLPZeO6557DZbGaXIuj3UdHo91Hx6HdSsej3UXaq3IRiERERcW/quRERERG3onAjIiIibkXhRkRERNyKwo2IiIi4FYWbMjJ16lRiY2Px8fGhbdu2LF++3OySqqxJkyZxxRVXEBgYSEREBAMHDmTHjh1mlyUnTZo0CYvFwpgxY8wupco6ePAgd999N6Ghofj5+XH55Zezbt06s8uqkvLz83n66aeJjY3F19eXevXqMXHiRAoLC80urVJTuCkDc+fOZcyYMYwfP57169fTrVs3+vXrR1xcnNmlVUnLli1j5MiRrFmzhkWLFpGfn0+fPn3IzMw0u7Qqb+3atUyfPp2WLVuaXUqVdezYMbp06YKXlxfff/89W7duZfLkyVSrVs3s0qqkl19+mXfeeYcpU6awbds2XnnlFV599VXeeusts0ur1HQreBno0KEDbdq0Ydq0aY59TZs2ZeDAgUyaNMnEygTgyJEjREREsGzZMq688kqzy6myMjIyaNOmDVOnTuWFF17g8ssv54033jC7rCrnqaeeYuXKlepdriCuu+46IiMjmTFjhmPfzTffjJ+fHx999JGJlVVu6rm5SLm5uaxbt44+ffo47e/Tpw+rVq0yqSo5U2pqKgAhISEmV1K1jRw5kmuvvZbevXubXUqVtmDBAtq1a8ett95KREQErVu35r333jO7rCqra9euLF68mJ07dwKwceNGVqxYQf/+/U2urHKrcgtnlrXk5GQKCgqIjIx02h8ZGUliYqJJVckphmEwduxYunbtSosWLcwup8r67LPP+PPPP1m7dq3ZpVR5e/bsYdq0aYwdO5Z//etf/P777zz88MPYbDaGDBlidnlVzpNPPklqaipNmjTBw8ODgoICXnzxRe644w6zS6vUFG7KiMVicXptGEaRfVL+Ro0axV9//cWKFSvMLqXKio+P55FHHuGnn37Cx8fH7HKqvMLCQtq1a8dLL70EQOvWrdmyZQvTpk1TuDHB3Llz+fjjj/n0009p3rw5GzZsYMyYMURHRzN06FCzy6u0FG4uUlhYGB4eHkV6aZKSkor05kj5Gj16NAsWLODXX3+lVq1aZpdTZa1bt46kpCTatm3r2FdQUMCvv/7KlClTyMnJwcPDw8QKq5YaNWrQrFkzp31Nmzblyy+/NKmiqu3xxx/nqaee4vbbbwfgsssuY//+/UyaNEnh5iJozs1F8vb2pm3btixatMhp/6JFi+jcubNJVVVthmEwatQovvrqK3755RdiY2PNLqlK69WrF5s2bWLDhg2OrV27dtx1111s2LBBwaacdenSpcijEXbu3EmdOnVMqqhqy8rKwmp1/ir28PDQreAXST03ZWDs2LEMHjyYdu3a0alTJ6ZPn05cXBwjRowwu7QqaeTIkXz66ad88803BAYGOnrVgoOD8fX1Nbm6qicwMLDIfCd/f39CQ0M1D8oEjz76KJ07d+all17itttu4/fff2f69OlMnz7d7NKqpAEDBvDiiy8SExND8+bNWb9+Pa+99hr33nuv2aVVboaUibffftuoU6eO4e3tbbRp08ZYtmyZ2SVVWUCx2wcffGB2aXJS9+7djUceecTsMqqsb7/91mjRooVhs9mMJk2aGNOnTze7pCorLS3NeOSRR4yYmBjDx8fHqFevnjF+/HgjJyfH7NIqNT3nRkRERNyK5tyIiIiIW1G4EREREbeicCMiIiJuReFGRERE3IrCjYiIiLgVhRsRERFxKwo3IiIi4lYUbkREsC9+O3/+fLPLEJEyoHAjIqYbNmwYFoulyHbNNdeYXZqIVEJaW0pEKoRrrrmGDz74wGmfzWYzqRoRqczUcyMiFYLNZiMqKsppq169OmAfMpo2bRr9+vXD19eX2NhY5s2b53T8pk2buOqqq/D19SU0NJQHHniAjIwMpzYzZ86kefPm2Gw2atSowahRo5zeT05O5sYbb8TPz4+GDRuyYMGCS/uhReSSULgRkUrhmWee4eabb2bjxo3cfffd3HHHHWzbtg2ArKwsrrnmGqpXr87atWuZN28eP//8s1N4mTZtGiNHjuSBBx5g06ZNLFiwgAYNGjhdY8KECdx222389ddf9O/fn7vuuoujR4+W6+cUkTJg9sqdIiJDhw41PDw8DH9/f6dt4sSJhmHYV3ofMWKE0zEdOnQw/vGPfxiGYRjTp083qlevbmRkZDje/9///mdYrVYjMTHRMAzDiI6ONsaPH3/OGgDj6aefdrzOyMgwLBaL8f3335fZ5xSR8qE5NyJSIfTs2ZNp06Y57QsJCXH83KlTJ6f3OnXqxIYNGwDYtm0brVq1wt/f3/F+ly5dKCwsZMeOHVgsFg4dOkSvXr1c1tCyZUvHz/7+/gQGBpKUlHShH0lETKJwIyIVgr+/f5FhovOxWCwAGIbh+Lm4Nr6+viU6n5eXV5FjCwsLS1WTiJhPc25EpFJYs2ZNkddNmjQBoFmzZmzYsIHMzEzH+ytXrsRqtdKoUSMCAwOpW7cuixcvLteaRcQc6rkRkQohJyeHxMREp32enp6EhYUBMG/ePNq1a0fXrl355JNP+P3335kxYwYAd911F8899xxDhw7l+eef58iRI4wePZrBgwcTGRkJwPPPP8+IESOIiIigX79+pKens3LlSkaPHl2+H1RELjmFGxGpEH744Qdq1KjhtK9x48Zs374dsN/J9Nlnn/HQQw8RFRXFJ598QrNmzQDw8/Pjxx9/5JFHHuGKK67Az8+Pm2++mddee81xrqFDh5Kdnc3rr7/OY489RlhYGLfcckv5fUARKTcWwzAMs4sQEXHFYrHw9ddfM3DgQLNLEZFKQHNuRERExK0o3IiIiIhb0ZwbEanwNHouIqWhnhsRERFxKwo3IiIi4lYUbkRERMStKNyIiIiIW1G4EREREbeicCMiIiJuReFGRERE3IrCjYiIiLgVhRsRERFxK/8PNUW5UlF1GOoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06318396, 0.07797826, 0.05831984, ..., 0.07010267, 0.07722056,\n",
       "        0.07111147],\n",
       "       [0.34888512, 0.35449427, 0.276049  , ..., 0.31110612, 0.28318927,\n",
       "        0.32426363],\n",
       "       [0.03704553, 0.0493429 , 0.03465981, ..., 0.04560534, 0.04959429,\n",
       "        0.04521822],\n",
       "       ...,\n",
       "       [0.04568898, 0.05947804, 0.04244432, ..., 0.05414167, 0.05909326,\n",
       "        0.05395456],\n",
       "       [0.29131502, 0.30035597, 0.25191402, ..., 0.29271623, 0.27917913,\n",
       "        0.30451345],\n",
       "       [0.17980929, 0.1979846 , 0.15998513, ..., 0.18687645, 0.19110239,\n",
       "        0.18944158]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(features_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model.save('my_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights on validation set: [0.00000000e+00 4.11380075e-03 7.12079423e-17 2.08911559e-18\n",
      " 1.80967302e-03 2.57604526e-04 2.20181452e-17 0.00000000e+00\n",
      " 0.00000000e+00 7.34618583e-03 3.23584076e-18 0.00000000e+00\n",
      " 7.40448428e-04 0.00000000e+00 1.01168589e-02 0.00000000e+00\n",
      " 0.00000000e+00 1.10978272e-02 5.56464725e-03 1.02900903e-02\n",
      " 1.81030560e-17 1.55678838e-03 6.52978832e-03 9.58596963e-18\n",
      " 0.00000000e+00 9.98065609e-18 1.20581669e-02 4.02817572e-03\n",
      " 0.00000000e+00 3.87523675e-03 3.15727889e-04 6.61947427e-18\n",
      " 0.00000000e+00 0.00000000e+00 1.36357406e-17 3.20258488e-03\n",
      " 0.00000000e+00 5.56744032e-03 8.85789142e-03 7.54781934e-03\n",
      " 4.71451098e-17 1.39593129e-03 1.20608813e-17 5.67497069e-17\n",
      " 2.55241846e-17 7.38529541e-03 6.77535619e-17 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.63951253e-03 2.42937585e-17\n",
      " 1.55747381e-02 4.38449489e-03 4.51025043e-19 2.89964845e-04\n",
      " 0.00000000e+00 5.80087366e-04 0.00000000e+00 0.00000000e+00\n",
      " 2.27332295e-03 1.52905097e-17 0.00000000e+00 2.82595520e-03\n",
      " 1.51738316e-17 1.45691885e-02 5.45396550e-03 3.36178245e-03\n",
      " 3.01140669e-18 0.00000000e+00 1.49697358e-17 1.02905671e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.04974865e-04\n",
      " 0.00000000e+00 3.01672191e-03 5.45242291e-03 3.56223796e-17\n",
      " 5.10370120e-18 0.00000000e+00 1.67868362e-18 0.00000000e+00\n",
      " 6.57226918e-18 0.00000000e+00 8.67172908e-04 0.00000000e+00\n",
      " 0.00000000e+00 1.64464378e-03 2.53662461e-03 1.37463081e-02\n",
      " 0.00000000e+00 3.35913023e-03 5.86484245e-18 7.36934730e-18\n",
      " 6.42125588e-03 0.00000000e+00 6.64582102e-17 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.00757516e-02 6.84437451e-03\n",
      " 5.93320400e-17 4.33579188e-03 0.00000000e+00 0.00000000e+00\n",
      " 6.62373933e-18 1.43623434e-03 1.99554151e-17 0.00000000e+00\n",
      " 7.75256303e-03 9.25868558e-04 0.00000000e+00 0.00000000e+00\n",
      " 2.11468886e-17 0.00000000e+00 1.19897479e-03 4.89949088e-04\n",
      " 7.57826216e-18 4.82159562e-18 0.00000000e+00 2.33206526e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.69889170e-03\n",
      " 0.00000000e+00 8.17170128e-04 1.66436018e-02 0.00000000e+00\n",
      " 2.77727947e-03 0.00000000e+00 0.00000000e+00 8.53309807e-04\n",
      " 6.56131138e-17 0.00000000e+00 8.19570572e-18 0.00000000e+00\n",
      " 0.00000000e+00 3.12340585e-04 3.58185057e-18 1.66390119e-03\n",
      " 4.35427961e-03 6.97785032e-18 0.00000000e+00 0.00000000e+00\n",
      " 1.02855411e-02 1.14507855e-04 0.00000000e+00 1.49357007e-17\n",
      " 2.79624505e-03 1.48340135e-03 1.12679013e-18 6.26282250e-03\n",
      " 0.00000000e+00 9.52714318e-18 2.62362139e-03 1.17925708e-17\n",
      " 4.98304891e-04 3.53351912e-03 1.31161703e-17 1.05909794e-02\n",
      " 7.22771368e-18 2.65703937e-03 4.54189276e-18 0.00000000e+00\n",
      " 7.30428125e-18 4.38754072e-04 2.59796653e-18 7.61756219e-03\n",
      " 0.00000000e+00 6.09594501e-03 7.42690750e-03 0.00000000e+00\n",
      " 0.00000000e+00 5.72665779e-03 0.00000000e+00 0.00000000e+00\n",
      " 1.99319409e-17 1.69926249e-17 3.83095276e-18 3.05242920e-03\n",
      " 1.64871112e-17 2.88797912e-18 5.25571487e-03 0.00000000e+00\n",
      " 1.83565277e-02 2.56170506e-02 0.00000000e+00 8.74701653e-18\n",
      " 2.28381114e-17 5.39107078e-03 2.61515767e-03 1.93645654e-03\n",
      " 1.46944650e-02 0.00000000e+00 7.56942994e-03 2.88571939e-03\n",
      " 9.54558070e-18 1.34141949e-18 2.16573178e-18 0.00000000e+00\n",
      " 1.98609760e-17 1.41696978e-17 6.47575078e-03 0.00000000e+00\n",
      " 9.69286310e-18 1.04706185e-17 0.00000000e+00 2.94021295e-17\n",
      " 5.78944242e-03 2.03277077e-17 0.00000000e+00 4.08801549e-18\n",
      " 6.60343004e-03 0.00000000e+00 4.71405134e-18 4.10750190e-18\n",
      " 7.35270771e-03 0.00000000e+00 8.85268198e-18 1.55805419e-17\n",
      " 3.48508854e-03 4.20368644e-03 0.00000000e+00 8.65215783e-03\n",
      " 0.00000000e+00 3.53062617e-18 6.20963507e-03 0.00000000e+00\n",
      " 7.41397975e-18 0.00000000e+00 3.71613988e-03 0.00000000e+00\n",
      " 4.04419636e-17 1.28130998e-02 6.79240163e-03 7.36873079e-03\n",
      " 3.08797814e-17 0.00000000e+00 4.16174920e-03 0.00000000e+00\n",
      " 1.85310193e-03 2.30424519e-02 0.00000000e+00 5.79574636e-03\n",
      " 1.78525333e-03 6.87361796e-03 0.00000000e+00 4.57310425e-03\n",
      " 0.00000000e+00 4.19881794e-03 9.24808626e-18 0.00000000e+00\n",
      " 8.40292401e-03 1.11084878e-02 6.61688444e-18 2.69426043e-03\n",
      " 1.06855482e-18 0.00000000e+00 3.84289616e-02 5.79592456e-17\n",
      " 1.38136596e-03 1.44084389e-18 0.00000000e+00 1.06108396e-18\n",
      " 2.83757715e-03 1.10005933e-17 1.06292023e-03 1.61956221e-02\n",
      " 0.00000000e+00 2.47923022e-18 8.48371852e-03 2.79374197e-03\n",
      " 7.07475447e-19 5.49509236e-03 0.00000000e+00 0.00000000e+00\n",
      " 1.91760655e-04 0.00000000e+00 3.46878489e-19 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.47218804e-19 2.54710463e-03\n",
      " 1.94046501e-17 3.72279624e-18 1.14752702e-02 3.68054560e-03\n",
      " 2.12382984e-03 5.78958981e-03 5.81901928e-03 0.00000000e+00\n",
      " 1.23484346e-17 1.89832652e-03 2.67894074e-03 3.99701743e-03\n",
      " 1.10080542e-02 9.41475273e-18 3.16655964e-03 5.72941494e-03\n",
      " 7.94369529e-03 5.10823197e-18 6.02946350e-03 1.77227659e-17\n",
      " 1.77621306e-02 7.39065001e-03 8.02570568e-03 6.49277423e-03\n",
      " 3.55303711e-03 5.99919130e-18 1.96250696e-04 3.55890426e-03\n",
      " 0.00000000e+00 5.97267485e-18 0.00000000e+00 7.70029228e-03\n",
      " 1.10185541e-17 1.27074525e-02 2.58042942e-03 1.54045758e-18\n",
      " 1.00929250e-03 0.00000000e+00 0.00000000e+00 5.86997497e-03\n",
      " 2.45799398e-03 0.00000000e+00 0.00000000e+00 1.01661509e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.17084442e-02 3.48291107e-03\n",
      " 5.14137054e-03 4.52568997e-03 1.70031970e-18 1.39163438e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.05936474e-18\n",
      " 3.37329322e-03 4.50129791e-17 7.98201950e-04 6.05020621e-18\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.53396146e-18\n",
      " 2.78831520e-03 6.40137545e-03 1.71478898e-03 1.35624458e-17\n",
      " 4.45265026e-03 0.00000000e+00 7.25852965e-04 2.81969438e-03\n",
      " 3.37985777e-17 5.95479634e-03 2.00938059e-03 0.00000000e+00\n",
      " 1.38462510e-03 2.03434991e-03 2.13876309e-18 0.00000000e+00\n",
      " 2.73235212e-02 1.42910805e-18 2.75723041e-03 5.54615126e-03\n",
      " 0.00000000e+00 1.71114905e-02 1.12478294e-17 4.94732559e-03\n",
      " 0.00000000e+00 2.58635373e-03 3.38079742e-03 1.13152482e-17\n",
      " 1.41370456e-02 6.80593601e-18 1.80204178e-17 0.00000000e+00\n",
      " 1.46568618e-17 7.85574589e-03 4.43464092e-18 2.48139496e-03\n",
      " 4.37387197e-03 1.99757503e-03 0.00000000e+00 0.00000000e+00\n",
      " 7.08068659e-19 3.92207974e-18 0.00000000e+00 5.11644406e-18\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Define the optimization function for the portfolio on the validation set\n",
    "def optimize_portfolio_val(weights, model, features_val, predictions_val, risk_aversion=0.8):\n",
    "    # Normalize weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Predict returns using the provided model\n",
    "    pred_returns = predictions_val\n",
    "    \n",
    "    # Calculate the covariance matrix from historical returns (you may not have this, so let's assume it's the identity matrix for simplicity)\n",
    "    num_assets = len(weights)\n",
    "    covariance_matrix = np.eye(num_assets)\n",
    "    \n",
    "    # Portfolio variance\n",
    "    portfolio_variance = np.dot(weights.T, np.dot(covariance_matrix, weights))\n",
    "\n",
    "    # Objective function: Maximize returns and minimize variance\n",
    "    objective = -np.dot(weights, pred_returns) + risk_aversion * portfolio_variance\n",
    "    return objective\n",
    "\n",
    "# Number of assets\n",
    "num_assets = targets_val.shape[1]\n",
    "\n",
    "# Initial guess for weights\n",
    "initial_weights = np.ones(num_assets) / num_assets\n",
    "\n",
    "# Constraints: sum of weights = 1\n",
    "constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "# Bounds for weights\n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "\n",
    "risk_aversion = 0.8\n",
    "# Calculate the average predicted returns across all samples in the validation set\n",
    "average_pred_returns = np.mean(predictions, axis=0)\n",
    "\n",
    "# Now use average_pred_returns instead of predictions_val in the optimization function\n",
    "result_val = minimize(optimize_portfolio_val, initial_weights, args=(model, features_val, average_pred_returns, risk_aversion),\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "print(\"Optimized weights on validation set:\", result_val.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = data.columns.get_level_values('Ticker').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Portfolio Weights (Test) (non-zero weights):\n",
      "AAPL: Weight 0.010689431248031556\n",
      "EQR: Weight 0.01495426249499892\n",
      "DTE: Weight 0.010388681112914327\n",
      "ILMN: Weight 0.010491161465071596\n",
      "TYL: Weight 0.017970350126053594\n",
      "HSY: Weight 0.014494341548389208\n",
      "PPG: Weight 0.013501863660149258\n",
      "GLW: Weight 0.01886126029644321\n",
      "CPT: Weight 0.01377711765172682\n",
      "IP: Weight 0.011104984694490179\n",
      "AKAM: Weight 0.011892662798594538\n",
      "FDS: Weight 0.015733276778723253\n",
      "JBL: Weight 0.011956491758710117\n",
      "SWK: Weight 0.026119055768573422\n",
      "ADI: Weight 0.011404502929237741\n",
      "MO: Weight 0.014961569136337529\n",
      "CB: Weight 0.019797934973220733\n",
      "DVN: Weight 0.011681952918956951\n",
      "HD: Weight 0.04764353084206584\n",
      "D: Weight 0.013861951097060387\n",
      "COR: Weight 0.015784540363489755\n",
      "PWR: Weight 0.01045167941615859\n",
      "YUM: Weight 0.010278368999495342\n",
      "KMB: Weight 0.010810526099874195\n",
      "BDX: Weight 0.013589667169493988\n",
      "MSI: Weight 0.029827102623244024\n",
      "Optimized Portfolio Weights with Tickers:\n",
      "A: Weight 0.005119791652255823\n",
      "AAPL: Weight 0.010689431248031556\n",
      "ABT: Weight 0.0\n",
      "ACGL: Weight 0.0012350374567799273\n",
      "ACN: Weight 0.003305138494076599\n",
      "ADBE: Weight 0.005769545846249768\n",
      "ADI: Weight 2.8132346458479467e-18\n",
      "ADM: Weight 0.004288332656262136\n",
      "ADP: Weight 0.002894443019578538\n",
      "ADSK: Weight 0.001840325179182051\n",
      "AEE: Weight 0.0\n",
      "AEP: Weight 0.0014527575238338021\n",
      "AES: Weight 6.120329204614553e-18\n",
      "AFL: Weight 0.0\n",
      "AIG: Weight 0.0069727573681151985\n",
      "AIZ: Weight 8.977278691474702e-18\n",
      "AJG: Weight 0.001614282218953813\n",
      "AKAM: Weight 0.0026483613043497946\n",
      "ALB: Weight 0.0\n",
      "ALGN: Weight 0.01495426249499892\n",
      "ALL: Weight 2.4819388843531085e-18\n",
      "AMAT: Weight 0.0022221070969023994\n",
      "AMD: Weight 0.010388681112914327\n",
      "AME: Weight 0.00649125744574366\n",
      "AMGN: Weight 0.010491161465071596\n",
      "AMT: Weight 0.0040651514753793005\n",
      "AMZN: Weight 0.017970350126053594\n",
      "ANSS: Weight 0.0011865581108615238\n",
      "AON: Weight 0.0010553602821627912\n",
      "AOS: Weight 8.136426437757398e-18\n",
      "APA: Weight 4.450509628221107e-18\n",
      "APD: Weight 0.0\n",
      "APH: Weight 2.7537835208625355e-18\n",
      "ARE: Weight 0.0035294453835075104\n",
      "ATO: Weight 0.005708296422119626\n",
      "AVB: Weight 5.042016558090426e-18\n",
      "AVY: Weight 1.1749485178940019e-17\n",
      "AXON: Weight 0.0051683077906779975\n",
      "AXP: Weight 0.0007111904412589816\n",
      "AZO: Weight 0.0\n",
      "BA: Weight 0.0024693054376531444\n",
      "BAC: Weight 5.38540900886324e-19\n",
      "BALL: Weight 0.004808021950486325\n",
      "BAX: Weight 0.007952110979591344\n",
      "BBWI: Weight 0.0\n",
      "BBY: Weight 0.001693592831920969\n",
      "BDX: Weight 0.004714091776307624\n",
      "BEN: Weight 4.606376925405699e-18\n",
      "BG: Weight 4.043774994861432e-18\n",
      "BIIB: Weight 0.00964783430970065\n",
      "BIO: Weight 0.0046831764389796545\n",
      "BK: Weight 1.6940923642882024e-18\n",
      "BKNG: Weight 0.014494341548389208\n",
      "BKR: Weight 0.004430339070216516\n",
      "BLK: Weight 0.001656004509091952\n",
      "BMY: Weight 4.87376140405224e-18\n",
      "BRO: Weight 3.2821070868226045e-18\n",
      "BSX: Weight 0.0006024844528280094\n",
      "BWA: Weight 0.0\n",
      "BXP: Weight 0.0013215983502839431\n",
      "C: Weight 0.0\n",
      "CAG: Weight 0.013501863660149258\n",
      "CAH: Weight 5.292539787130686e-18\n",
      "CAT: Weight 2.0728232943182615e-17\n",
      "CB: Weight 0.0025663358518521993\n",
      "CBRE: Weight 1.1140468489864177e-18\n",
      "CCI: Weight 3.658427943419929e-18\n",
      "CCL: Weight 0.007021442104864067\n",
      "CDNS: Weight 0.0\n",
      "CHD: Weight 0.0043856543968469365\n",
      "CHRW: Weight 0.0\n",
      "CI: Weight 0.0026162762455890945\n",
      "CINF: Weight 0.00624446267153468\n",
      "CL: Weight 0.008194238504339512\n",
      "CLX: Weight 0.002672977650596229\n",
      "CMA: Weight 0.0\n",
      "CMCSA: Weight 0.0\n",
      "CME: Weight 0.000648054511425673\n",
      "CMI: Weight 0.0\n",
      "CMS: Weight 0.0029832457867533624\n",
      "CNC: Weight 0.01886126029644321\n",
      "CNP: Weight 0.0\n",
      "COF: Weight 6.949931802001136e-18\n",
      "COO: Weight 0.0\n",
      "COP: Weight 0.0\n",
      "COR: Weight 0.0\n",
      "COST: Weight 7.223430799735669e-18\n",
      "CPB: Weight 0.00147945109828311\n",
      "CPRT: Weight 0.0033704394476822192\n",
      "CPT: Weight 0.00276792361621126\n",
      "CRL: Weight 0.002742295203818747\n",
      "CRM: Weight 0.008028139943263773\n",
      "CSCO: Weight 3.419247014938996e-18\n",
      "CSGP: Weight 0.0034951037801144364\n",
      "CSX: Weight 0.0\n",
      "CTAS: Weight 2.37526567255827e-18\n",
      "CTRA: Weight 0.0014553480054907888\n",
      "CTSH: Weight 0.000516849232146438\n",
      "CVS: Weight 0.0\n",
      "CVX: Weight 1.4620146011633847e-17\n",
      "D: Weight 1.5301782541046944e-18\n",
      "DD: Weight 0.0040193357357259625\n",
      "DE: Weight 0.0\n",
      "DECK: Weight 0.0029147898647711923\n",
      "DGX: Weight 0.0\n",
      "DHI: Weight 0.002877610466220019\n",
      "DHR: Weight 1.249982402130932e-18\n",
      "DIS: Weight 2.506873416737907e-18\n",
      "DLR: Weight 0.0016613850477321644\n",
      "DLTR: Weight 0.01377711765172682\n",
      "DOC: Weight 0.004174513701589266\n",
      "DOV: Weight 0.0\n",
      "DPZ: Weight 0.004127480732527901\n",
      "DRI: Weight 0.0\n",
      "DTE: Weight 0.0002854350465037369\n",
      "DUK: Weight 0.0\n",
      "DVA: Weight 0.00035763627041427977\n",
      "DVN: Weight 0.0\n",
      "EA: Weight 3.978328464796237e-18\n",
      "EBAY: Weight 0.0\n",
      "ECL: Weight 0.0012328780209428108\n",
      "ED: Weight 0.008102665708878657\n",
      "EFX: Weight 2.866902123990387e-18\n",
      "EG: Weight 0.002087572890553088\n",
      "EIX: Weight 0.0024308776051258792\n",
      "EL: Weight 0.0\n",
      "ELV: Weight 0.0026560642918494123\n",
      "EMN: Weight 3.814136421926911e-18\n",
      "EMR: Weight 0.0\n",
      "EOG: Weight 0.001051519734084249\n",
      "EQIX: Weight 0.005923660742901209\n",
      "EQR: Weight 0.001013132189151142\n",
      "EQT: Weight 0.0\n",
      "ES: Weight 0.0006542708344739056\n",
      "ESS: Weight 2.009704781369035e-18\n",
      "ETN: Weight 0.0\n",
      "ETR: Weight 0.004995783787802472\n",
      "EVRG: Weight 0.0015812183914599293\n",
      "EW: Weight 0.0\n",
      "EXC: Weight 0.001566354781638165\n",
      "EXPD: Weight 5.152779350833727e-18\n",
      "EXR: Weight 0.0014118658905675897\n",
      "F: Weight 0.0\n",
      "FAST: Weight 4.520945711741222e-18\n",
      "FCX: Weight 0.0\n",
      "FDS: Weight 0.00032037792911996597\n",
      "FDX: Weight 0.002739607613366545\n",
      "FE: Weight 6.28313158405479e-18\n",
      "FFIV: Weight 0.011104984694490179\n",
      "FI: Weight 0.0001771580110479612\n",
      "FICO: Weight 0.0\n",
      "FIS: Weight 0.0\n",
      "FITB: Weight 0.0013026208018502663\n",
      "FMC: Weight 0.00463571428674347\n",
      "FRT: Weight 0.0\n",
      "GD: Weight 0.011892662798594538\n",
      "GE: Weight 4.153995157122898e-18\n",
      "GEN: Weight 0.0\n",
      "GILD: Weight 0.005005952668885792\n",
      "GIS: Weight 0.0033507781821916264\n",
      "GL: Weight 1.4060879273319394e-18\n",
      "GLW: Weight 0.0015074131354058241\n",
      "GOOG: Weight 0.015733276778723253\n",
      "GOOGL: Weight 0.0\n",
      "GPC: Weight 1.3968896789203497e-18\n",
      "GPN: Weight 0.004190070571893355\n",
      "GRMN: Weight 0.003850892374049237\n",
      "GS: Weight 9.846387434916818e-18\n",
      "GWW: Weight 0.006593780178339614\n",
      "HAL: Weight 2.5701494248757633e-18\n",
      "HAS: Weight 1.0342404634873016e-18\n",
      "HBAN: Weight 2.069089731905442e-18\n",
      "HD: Weight 0.0019557884535927354\n",
      "HES: Weight 0.011956491758710117\n",
      "HIG: Weight 0.004158509712257844\n",
      "HOLX: Weight 0.0030156075009383425\n",
      "HON: Weight 0.005839528950477149\n",
      "HPQ: Weight 0.0046268525234824465\n",
      "HRL: Weight 0.008037093460215416\n",
      "HSIC: Weight 0.0\n",
      "HST: Weight 1.5238122721104863e-18\n",
      "HSY: Weight 0.0006956081845216758\n",
      "HUBB: Weight 0.0\n",
      "HUM: Weight 0.00870877104963674\n",
      "IBM: Weight 3.293546002077478e-17\n",
      "IDXX: Weight 0.007475045664140348\n",
      "IEX: Weight 0.001425829444199697\n",
      "IFF: Weight 3.7685025218935856e-19\n",
      "ILMN: Weight 0.026119055768573422\n",
      "INCY: Weight 0.004464328798101389\n",
      "INTC: Weight 0.0046448254563505684\n",
      "INTU: Weight 0.00019501357622378647\n",
      "IP: Weight 0.0\n",
      "IPG: Weight 0.0\n",
      "IRM: Weight 0.0\n",
      "ISRG: Weight 0.0062433316026199335\n",
      "IT: Weight 0.0\n",
      "ITW: Weight 3.193340680928314e-18\n",
      "IVZ: Weight 5.034724133810159e-18\n",
      "J: Weight 0.0\n",
      "JBHT: Weight 0.0\n",
      "JBL: Weight 0.0\n",
      "JCI: Weight 1.4311283388351447e-18\n",
      "JKHY: Weight 1.564099276664269e-19\n",
      "JNJ: Weight 2.5201612460979588e-18\n",
      "JNPR: Weight 5.992228706232141e-19\n",
      "JPM: Weight 2.415168865307828e-18\n",
      "K: Weight 0.0\n",
      "KEY: Weight 0.0\n",
      "KIM: Weight 0.0009436239134285632\n",
      "KLAC: Weight 4.9027061080466955e-18\n",
      "KMB: Weight 0.005346375345216094\n",
      "KMX: Weight 0.0\n",
      "KO: Weight 9.39796289868447e-19\n",
      "KR: Weight 0.007532027452493069\n",
      "L: Weight 4.981784574606765e-18\n",
      "LEN: Weight 0.002986208169984726\n",
      "LH: Weight 0.004089500246596477\n",
      "LHX: Weight 0.006843845425971968\n",
      "LIN: Weight 9.304275223065652e-05\n",
      "LKQ: Weight 0.001298954442664929\n",
      "LLY: Weight 0.0003327816936927322\n",
      "LMT: Weight 0.004815101997619804\n",
      "LNT: Weight 0.011404502929237741\n",
      "LOW: Weight 1.6414439726604273e-18\n",
      "LRCX: Weight 0.0031209973706156214\n",
      "LUV: Weight 0.005772147273578033\n",
      "LVS: Weight 0.014961569136337529\n",
      "MAA: Weight 2.4259817702751213e-18\n",
      "MAR: Weight 0.001096349254423948\n",
      "MAS: Weight 4.548183114951368e-18\n",
      "MCD: Weight 0.0035121158801019567\n",
      "MCHP: Weight 1.6095478882403825e-18\n",
      "MCK: Weight 0.006554707051073448\n",
      "MCO: Weight 0.0010412407056895631\n",
      "MDLZ: Weight 0.0020431467065556933\n",
      "MDT: Weight 0.004747047546703935\n",
      "MET: Weight 3.3818319814719975e-18\n",
      "MGM: Weight 0.0032235638858964626\n",
      "MHK: Weight 0.0\n",
      "MKC: Weight 0.0018487617147582398\n",
      "MKTX: Weight 0.005696004667503231\n",
      "MLM: Weight 0.0\n",
      "MMC: Weight 3.469473421733216e-18\n",
      "MMM: Weight 0.0\n",
      "MNST: Weight 0.019797934973220733\n",
      "MO: Weight 0.0\n",
      "MOH: Weight 0.0002806944422354756\n",
      "MOS: Weight 8.998269226698848e-19\n",
      "MPWR: Weight 0.011681952918956951\n",
      "MRK: Weight 0.0031537936257111858\n",
      "MRO: Weight 0.007088631196618233\n",
      "MS: Weight 4.982406614427405e-19\n",
      "MSI: Weight 1.901866899271722e-18\n",
      "MTB: Weight 0.0\n",
      "MTCH: Weight 0.00228026046670201\n",
      "MTD: Weight 0.0015645616594401964\n",
      "MU: Weight 1.9407642403964157e-19\n",
      "NDAQ: Weight 0.001309337800235817\n",
      "NDSN: Weight 0.0\n",
      "NEE: Weight 0.0052918829742351755\n",
      "NEM: Weight 0.0\n",
      "NFLX: Weight 0.04764353084206584\n",
      "NI: Weight 0.0002658985992859991\n",
      "NKE: Weight 0.0\n",
      "NOC: Weight 0.0018178170355071192\n",
      "NRG: Weight 0.0032355515058106053\n",
      "NSC: Weight 0.0\n",
      "NTAP: Weight 0.0\n",
      "NTRS: Weight 0.0\n",
      "NUE: Weight 3.493825618966777e-18\n",
      "NVDA: Weight 0.004085622906014268\n",
      "NVR: Weight 0.0\n",
      "O: Weight 0.0\n",
      "ODFL: Weight 0.0030139759895168898\n",
      "OKE: Weight 0.005450623200307215\n",
      "OMC: Weight 0.002652210935454434\n",
      "ON: Weight 0.0003996536122285552\n",
      "ORCL: Weight 1.4751740520923683e-18\n",
      "ORLY: Weight 0.006411316439198799\n",
      "OXY: Weight 0.0\n",
      "PAYX: Weight 4.53947455746241e-18\n",
      "PCAR: Weight 0.0\n",
      "PCG: Weight 2.0958771488623594e-18\n",
      "PEG: Weight 0.005609174338955857\n",
      "PEP: Weight 0.0\n",
      "PFE: Weight 0.0029081187419154313\n",
      "PFG: Weight 1.7516509000320923e-18\n",
      "PG: Weight 0.0\n",
      "PGR: Weight 0.0\n",
      "PH: Weight 4.221003404184594e-18\n",
      "PHM: Weight 1.9022639459657475e-18\n",
      "PKG: Weight 0.005759454042633534\n",
      "PLD: Weight 0.0\n",
      "PNC: Weight 8.022328452784284e-19\n",
      "PNR: Weight 0.0\n",
      "PNW: Weight 1.615715346887578e-19\n",
      "POOL: Weight 0.0008456091425611206\n",
      "PPG: Weight 6.878661920423567e-18\n",
      "PPL: Weight 6.8738311856462574e-18\n",
      "PRU: Weight 1.6259062120342312e-19\n",
      "PSA: Weight 1.1949120256696017e-18\n",
      "PTC: Weight 0.0\n",
      "PWR: Weight 0.0\n",
      "PXD: Weight 1.970437914657964e-05\n",
      "QCOM: Weight 0.003167460265022002\n",
      "RCL: Weight 6.803792148820167e-19\n",
      "REG: Weight 0.0\n",
      "REGN: Weight 0.013861951097060387\n",
      "RF: Weight 3.731458065341011e-18\n",
      "RHI: Weight 0.0\n",
      "RJF: Weight 8.408403423164834e-18\n",
      "RL: Weight 0.0\n",
      "RMD: Weight 0.004886186826208059\n",
      "ROK: Weight 5.810857776401314e-18\n",
      "ROL: Weight 0.0\n",
      "ROP: Weight 0.0007901708424917399\n",
      "ROST: Weight 0.0008949154200611828\n",
      "RSG: Weight 0.0\n",
      "RTX: Weight 0.0\n",
      "RVTY: Weight 0.0\n",
      "SBAC: Weight 0.005791368410222598\n",
      "SBUX: Weight 0.0\n",
      "SCHW: Weight 0.0\n",
      "SHW: Weight 7.297188840595797e-19\n",
      "SJM: Weight 0.015784540363489755\n",
      "SLB: Weight 0.0\n",
      "SNA: Weight 1.3572908886362112e-18\n",
      "SNPS: Weight 0.005811968387051634\n",
      "SO: Weight 0.0\n",
      "SPG: Weight 0.0\n",
      "SPGI: Weight 1.1442488675119538e-18\n",
      "SRE: Weight 8.512692068898829e-05\n",
      "STE: Weight 0.0042890366322160724\n",
      "STLD: Weight 0.0\n",
      "STT: Weight 5.67211642661902e-18\n",
      "STX: Weight 0.01045167941615859\n",
      "STZ: Weight 0.001574593144133938\n",
      "SWK: Weight 0.007462002425594886\n",
      "SWKS: Weight 1.2759492559201967e-18\n",
      "SYK: Weight 0.004276402087748671\n",
      "SYY: Weight 1.7426511749675153e-18\n",
      "T: Weight 0.0\n",
      "TAP: Weight 0.009670615613709159\n",
      "TDY: Weight 0.000481653448093451\n",
      "TECH: Weight 0.010278368999495342\n",
      "TER: Weight 1.8620960554201725e-18\n",
      "TFC: Weight 2.8388573925023893e-18\n",
      "TFX: Weight 0.010810526099874195\n",
      "TGT: Weight 4.107448049693314e-19\n",
      "TJX: Weight 0.00022098664111242855\n",
      "TMO: Weight 0.0019593651545633277\n",
      "TPR: Weight 1.5276106854833298e-18\n",
      "TRMB: Weight 0.0019355773723441292\n",
      "TROW: Weight 1.6201093303014596e-18\n",
      "TRV: Weight 0.0\n",
      "TSCO: Weight 0.007097019994019575\n",
      "TSN: Weight 0.0025386982083286432\n",
      "TT: Weight 0.0\n",
      "TTWO: Weight 0.0031085295618299043\n",
      "TXN: Weight 0.002981033024805978\n",
      "TXT: Weight 2.926300309416595e-18\n",
      "TYL: Weight 0.013589667169493988\n",
      "UDR: Weight 0.0\n",
      "UHS: Weight 0.004722326413513013\n",
      "UNH: Weight 0.0005181197850336772\n",
      "UNP: Weight 0.002407261918955078\n",
      "UPS: Weight 6.703116004976121e-06\n",
      "URI: Weight 0.003137575159892257\n",
      "USB: Weight 1.897406741408836e-18\n",
      "VLO: Weight 0.001298809595694776\n",
      "VMC: Weight 2.8826913475227993e-18\n",
      "VRSN: Weight 0.00046329743777999557\n",
      "VRTX: Weight 0.029827102623244024\n",
      "VTR: Weight 4.917026258811213e-19\n",
      "VTRS: Weight 0.009383609200631767\n",
      "VZ: Weight 1.6351176953356217e-18\n",
      "WAB: Weight 0.0\n",
      "WAT: Weight 0.003294443551222892\n",
      "WBA: Weight 0.0029581234074339095\n",
      "WDC: Weight 0.00017683152460389354\n",
      "WEC: Weight 0.0020077527494222385\n",
      "WELL: Weight 0.0\n",
      "WFC: Weight 0.0\n",
      "WM: Weight 0.0028893118872843665\n",
      "WMB: Weight 6.4239243417562155e-18\n",
      "WMT: Weight 0.008807337498907284\n",
      "WRB: Weight 0.0\n",
      "WST: Weight 0.005266821144037961\n",
      "WTW: Weight 1.0480709233291882e-18\n",
      "WY: Weight 0.0006523424087311678\n",
      "WYNN: Weight 0.003402319112981521\n",
      "XEL: Weight 0.00477446489823578\n",
      "XOM: Weight 0.0\n",
      "YUM: Weight 2.034599609084431e-19\n",
      "ZBH: Weight 0.0036683371166796584\n",
      "ZBRA: Weight 0.0\n"
     ]
    }
   ],
   "source": [
    "# Zip result.x with tickers to create a list of tuples containing ticker symbols and corresponding weights\n",
    "ticker_weights = zip(tickers, result.x)\n",
    "\n",
    "# Convert the zipped result to a dictionary for easier manipulation\n",
    "ticker_weights_dict = dict(ticker_weights)\n",
    "\n",
    "# Print the non-zero weights\n",
    "print_non_zero_weights(result.x, \"Optimized Portfolio Weights (Test)\", symbols)\n",
    "\n",
    "# Display the ticker symbols with non-zero weights\n",
    "print(\"Optimized Portfolio Weights with Tickers:\")\n",
    "for ticker, weight in ticker_weights_dict.items():\n",
    "    print(f\"{ticker}: Weight {weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profit/Loss from optimized portfolio: 29.15335306770976\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract Opening Prices\n",
    "get_prices = pd.read_csv('data/raw_data_yf.csv')\n",
    "\n",
    "# Filter the DataFrame for the specified date and select 'Ticker' and 'Open' columns\n",
    "opening_prices = get_prices[get_prices['Date'] == '2022-01-01'][['Ticker', 'Open']]\n",
    "\n",
    "# Step 2: Calculate Allocation\n",
    "initial_investment = 10000  # Initial investment amount\n",
    "proposed_weights = result.x  # Proposed weights from optimization\n",
    "\n",
    "# Merge the opening prices DataFrame with the ticker weights to ensure alignment\n",
    "merged_data = pd.merge(opening_prices, pd.DataFrame(ticker_weights_dict.items(), columns=['Ticker', 'Weight']), on='Ticker')\n",
    "\n",
    "# Calculate allocation for each asset\n",
    "merged_data['Allocation'] = (initial_investment * merged_data['Weight']) / merged_data['Open']\n",
    "\n",
    "# Step 3:  Evaluate Portfolio Value\n",
    "# Assuming closing_prices is a DataFrame containing closing prices for each ticker on 12-31-2023\n",
    "closing_prices = get_prices[get_prices['Date'] == '2023-12-23'][['Ticker', 'Close']]\n",
    "\n",
    "# Merge the allocation data with the closing prices\n",
    "merged_data = pd.merge(merged_data, closing_prices, on='Ticker')\n",
    "\n",
    "# Calculate the total value of the portfolio\n",
    "portfolio_value = (merged_data['Allocation'] * merged_data['Close']).sum()\n",
    "\n",
    "# Step 4: Calculate Profit\n",
    "profit = portfolio_value - initial_investment\n",
    "\n",
    "# Step 5: Compare\n",
    "print(\"Profit/Loss from optimized portfolio:\", profit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import pyepo\n",
    "from pyepo.model.grb import optGrbModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "m = targets_val.shape[1] # total number of assets\n",
    "cov = np.cov(np.random.randn(10, m), rowvar=False) # covariance matrix\n",
    "optmodel = pyepo.model.grb.portfolioModel(m, cov) # build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "def build_model(num_features, num_time_steps, output_shape):\n",
    "    model = Sequential()\n",
    "    # Add a LSTM layer only if your data is sequenced; otherwise start with Dense\n",
    "    model.add(LSTM(64, activation = 'relu', input_shape=(num_time_steps, num_features), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, activation = 'relu', return_sequences=False))\n",
    "    model.add(Dense(output_shape, activation='sigmoid'))  # Sigmoid or softmax based on your needs\n",
    "    return model\n",
    "\n",
    "# Example usage assuming you've correctly shaped your data\n",
    "num_features = features_train.shape[2]  # number of features per timestep\n",
    "num_time_steps = features_train.shape[1]  # number of timesteps per sample\n",
    "output_shape = targets_train.shape[1]  # assuming this is a regression task or multi-label classification\n",
    "lstm = build_model(num_features, num_time_steps,output_shape)\n",
    "\n",
    "spop = pyepo.func.SPOPlus(optmodel, processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# train model\n",
    "def trainModel(reg, loss_func, method_name, num_epochs=20, lr=1e-2):\n",
    "    # set adam optimizer\n",
    "    optimizer = torch.optim.Adam(reg.parameters(), lr=lr)\n",
    "    # train mode\n",
    "    reg.train()\n",
    "    # init log\n",
    "    train_loss_log = []\n",
    "    loss_log_regret = [pyepo.metric.regret(reg, optmodel, val_loader)] #use val_loader\n",
    "    # init elpased time\n",
    "    elapsed = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # start timing\n",
    "        tick = time.time()\n",
    "        # load data\n",
    "        train_loss = 0\n",
    "        for i, data in enumerate(train_loader): #train_loader\n",
    "            x, c, w, z = data\n",
    "            # cuda\n",
    "            if torch.cuda.is_available():\n",
    "                x, c, w, z = x.cuda(), c.cuda(), w.cuda(), z.cuda()\n",
    "            # forward pass\n",
    "            cp = reg(x)\n",
    "            if method_name == \"spo+\":\n",
    "                loss = loss_func(cp, c, w, z)\n",
    "            elif method_name == \"mse\":\n",
    "                loss = loss_func(cp, c)\n",
    "            else:\n",
    "                raise ValueError(\"Method name {} not supported\".format(method_name))\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # record time\n",
    "            tock = time.time()\n",
    "            elapsed += tock - tick\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader) #training still\n",
    "        train_loss_log.append(train_loss)\n",
    "        regret = pyepo.metric.regret(reg, optmodel, val_loader) #regret calculated on val set\n",
    "        loss_log_regret.append(regret)\n",
    "        print(\"Epoch {:2},  Loss: {:9.4f},  Regret: {:7.4f}%\".format(epoch+1, train_loss, regret*100))\n",
    "    print(\"Total Elapsed Time: {:.2f} Sec.\".format(elapsed))\n",
    "    return train_loss_log, loss_log_regret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = \"spo+\"\n",
    "\n",
    "# Instantiate the model\n",
    "loss_log_lstm_spo, loss_log_regret_lstm_spo = trainModel(lstm, loss_func=spop, method_name=method_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###QUESTION FOR HYUNGKI: HOW DO I GET THE PREDICTIONS AFTER THIS. RUN MODEL ON TEST_LOADER\n",
    "#need to see what weights we decided to assign after the training with NN, which isn't shown in the documentation or lab code\n",
    "#thoughts: optmodel.solve?\n",
    "# Then how do I use it to evaluate, just multiply?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
